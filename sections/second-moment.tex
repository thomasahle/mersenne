
%! TEX root = ../mersenne.tex

\section{Analysis of second moment estimation using Mersenne primes}
\label{sec:analysis-two-for-one}
In this section, we will prove Theorem \ref{thm:h-and-s-p}---that a single Mersenne hash function works for Count Sketch.
Recall that for each key $x\in [u]$, we have a value $f_x\in \Z$, and the
goal was to estimate the second moment $F_2 = \sum_{x\in u}f_x^2$.

We had two functions $i:[u]\to[r]$ and $s:[u]\to\{-1,1\}$. 
For notational convenience, we define $i_x=i(x)$ and $s_x=s(x)$.
We let $r=2^\ell>1$ and $u>r$ both be powers of two and $p=2^b-1>u$ a Mersenne prime.
For each $i\in [r]$, we have a counter 
$C_i=\sum_{x\in[u]} s_x f_x[i_x=i]$, and we define the 
estimator $X=\sum_{i\in[r]} C_i^2$. We want to study how
well it approximates $F_2$.
We have 
\begin{align}
X=\sum_{i\in[r]}\left( \sum_{x\in[u]}s_x f_x[i_x=i]\right)^2
%=\sum_{i\in[r]}\sum_{x,y\in[u]}s_x s_y f_x f_y [i_x = i_y = i]
=\sum_{x,y\in[u]}s_x s_y f_x f_y[i_x=i_y]
=\sum_{x\in[u]} f_x^2+Y,
\label{eq:decomp}
\end{align}
where $Y=\sum_{x,y\in[u],x\neq y} s_x s_y f_x f_y [i_x = i_y]$.
The goal is thus to bound mean and variance of the error $Y$.

As discussed in the introduction, one of the critical steps in the analysis of count sketch in the classical case is \cref{eq:E-0}.
We formalize this into the following property:
\begin{property}[Sign Cancellation]\label{prop:independence}
    For distinct keys $x_0, \ldots x_{j - 1}$, $j \le k$
    and an expression $A(i_{x_0}, \ldots, i_{x_{j - 1}}, s_{x_1}, \ldots, s_{x_{j - 1}})$,
    which depends on $i_{x_0}, \ldots, i_{x_{j - 1}}$ and $s_{x_1}, \ldots, s_{x_{j - 1}}$
    but not on $s_{x_0}$
    \begin{align}
        \E[s_{x_0} A(i_{x_0}, \ldots, i_{x_{j - 1}}, s_{x_1}, \ldots, s_{x_{j - 1}})] = 0\; .
    \end{align}
\end{property}

In the case where we use a Mersenne prime for our hash function we have that $h$ is uniform in $[2^b - 1]$ and not in $[2^b]$, hence \Cref{prop:independence} is not satisfied.
Instead, we have \cref{eq:E-0} which is almost as good, and will replace \Cref{prop:independence} in the analysis for count sketch.
We formalize this as follows:
\begin{property}[Sign Near Cancellation]\label{prop:near-independence}
    Given $k, p$ and $\delta$,
    there exists $t \in [r]$ such that for distinct keys $x_0, \ldots x_{j - 1}$, $j \le k$
    and an expression $A(i_{x_0}, \ldots, i_{x_{j - 1}}, s_{x_1}, \dots, s_{x_{j - 1}})$,
    which depends on $i_{x_0}, \ldots, i_{x_{j - 1}}$
    and $s_{x_1}, \ldots, s_{x_{j - 1}}$,
    but not on $s_{x_0}$,
    \begin{align}
        \E[s_{x_0} A(i_{x_0}, \ldots, i_{x_{j - 1}}, s_{x_1}, \ldots, s_{x_{j - 1}})]
            &= \frac1p \E[A(i_{x_0}, \ldots, i_{x_{j - 1}}, s_{x_1}, \ldots, s_{x_{j - 1}}) \mid i_{x_0} = t].
         \label{eq:near-independence}
            \\
            \text{and}\quad
    \Pr[i_x = t] &\le (1 + \delta)/r
    \quad\text{for any key $x$}.
         \label{eq:prob-special-value}
    \end{align}
\end{property}

When the hash function $h$ is not uniform then it is not guaranteed that
the collision probability is $1/r$, but \eqref{eq:coll} showed that for
Mersenne primes the collision probability is $(1 + (r - 1)/p^2)/r$.
We formalize this into the following property.
\begin{property}[Low Collisions]\label{prop:collision}
   We say the hash function has $(1+\eps)/r$-low collision probability, if
    for distinct keys $x \neq y$,
    \begin{align}\label{eq:collision}
        \Pr[i_x = i_y] \le (1 + \eps)/r\; .
    \end{align}
\end{property}

\subsection{The analysis in the classical case}
First, as a warm-up for later comparison, we analyse the
case where we have Sign Cancellation, but
the collision probability bound is only $(1+\eps)/r$.
This will come in useful in \Cref{sec:arbitrary-buckets} where we will consider the case of an arbitrary number of buckets, not necessarily a power of two.
\begin{lemma}\label{lem:count-classic}
   If the hash function has Sign Cancellation for $k = 4$ and $(1+\eps)/r$-low collision probability, then
    \begin{align}
        \E[X] &= F_2 \\
        \Var[X] &\le 2(1 + \eps)(F_2^2 - F_4)/r \le 2(1 + \eps)F_2^2/r .
    \end{align}
\end{lemma}
\begin{proof}
   Recall the decomposition $X=F_2+Y$ from \cref{eq:decomp}.
    We will first show that $\E[Y] = 0$.
    By \Cref{prop:independence} we have that $\E[s_x s_y f_x f_y [i_x = i_y]] = 0$
    for $x \neq y$ and thus $\E[Y] = \sum_{x,y\in[u],x\neq y} \E[s_x s_y f_x f_y [i_x = i_y]] = 0$.

    Now we want to bound the variance of $X$. We note that since $\E[Y] = 0$ and $X = F_2 + Y$
    \begin{align*}
        \Var[X] = \Var[Y] = \E[Y^2]
            = \sum_{\substack{x, y, x', y' \in [u]\\ x \neq y, x' \neq y'}} \E[(s_x s_y f_x f_y [i_x = i_y])(s_{x'} s_{y'} f_{x'} f_{y'} [i_{x'} = i_{y'}])] .
    \end{align*}
    Now we consider one of the terms $\E[(s_x s_y f_x f_y [i_x = i_y])(s_{x'} s_{y'} f_{x'} f_{y'} [i_{x'} = i_{y'}])]$.
    Suppose that one of the keys, say $x$, is unique, i.e. $x \not\in \{y, x', y'\}$.
    Then the Sign Cancellation Property implies that 
    \[
        \E[(s_x s_y f_x f_y [i_x = i_y])(s_{x'} s_{y'} f_{x'} f_{y'} [i_{x'} = i_{y'}])] = 0 .
    \]
    Thus we can now assume that there are no unique keys. Since $x \neq y$ and $x' \neq y'$, we conclude
    that $(x, y) = (x', y')$ or $(x, y) = (y', x')$. Therefore
    \begin{align*}
       \Var[X] &= \sum_{\substack{x, y, x', y' \in [u]\\ x \neq y, x' \neq y'}}
                \E[(s_x s_y f_x f_y [i_x = i_y])(s_{x'} s_{y'} f_{x'} f_{y'} [i_{x'} = i_{y'}])]
            \\&= 2 \sum_{\substack{x, y, x', y' \in [u]\\ x \neq y, (x', y') = (x, y)}}
                \E[(s_x s_y f_x f_y [i_x = i_y])(s_{x'} s_{y'} f_{x'} f_{y'} [i_{x'} = i_{y'}])]
            \\&= 2\sum_{x,y\in[u],x\neq y} \E[(s_x s_y f_x f_y[i_x=i_y])^2]
            \\&= 2\sum_{x,y\in[u],x\neq y} \E[(f_x^2f_y^2[i_x=i_y])]
            \\&\le 2\sum_{x,y\in[u],x\neq y} (f_x^2f_y^2)(1 + \eps)/r
            \\&= 2(1 + \eps) (F_2^2-F_4)/r.
    \end{align*}
    The inequality follows by \Cref{prop:collision}.
\end{proof}
%Something something finish this part\todo{Write this properly}
%This completes the proof of \eqref{eq:V-F2}.
In the above analysis, we
did not need $s$ and $i$ to be completely independent. All we needed
was that for any $j\leq 4$ distinct keys $x_0,\ldots,x_{j - 1}$, the hash
values $s(x_0),\ldots,s(x_{j - 1})$ and $i(x_0),\ldots,i(x_{j - 1})$ are all
independent and uniform in the desired domain. This was why we could
use a single 4-universal hash function $h:[u]\to[2^b]$ with
$b>\ell$, and use it to construct $s:[u]\to\{-1, 1\}$ and
$i:[u]\to[2^\ell]$ as described in Algorithm \ref{alg:h-and-s}.
% This was removed from the introduction
%(c.f. Lemma \ref{lem:b-bit-hashing}).

\subsection{The analysis of two-for-one using Mersenne primes}
We will now analyse the case where the functions $s : [u] \to \{-1, 1\}$
and $i : [u] \to [2^l]$ are constructed as in \Cref{alg:Mersenne} from a
single $k$-universal hash function $h : [u] \to [2^b - 1]$ where $2^b - 1$
is a Mersenne prime.
We now only have \nameref{prop:near-independence}.
We will show that this does
not change the expectation and variance too much. Similarly, to the
analysis of the classical case, we will analyse a slightly more general
problem, which will be useful in \Cref{sec:arbitrary-buckets}.
\begin{lemma}\label{lem:count-mersenne}
   If we have \nameref{prop:near-independence} with $
    \Pr[i_x = t] \le (1 + \delta)/r$
   and $(1+\eps)/r$-low collision probability,
   then
    \begin{align}
        \E[X] &= F_2 + (F_1^2 - F_2)/p^2 \\
        | \E[X] - F_2 | &\le F_2 (n - 1)/p^2 \\
        \Var[X] &\le 2F_2^2/r + F_2^2 (2\eps/r + 4(1 + \delta)n / (rp^2) + n^2/p^4 - 2 /(rn))
    \end{align}
\end{lemma}
\begin{proof}
    We first bound $\E[s_x s_y f_x f_y [i_x = i_y]]$ for distinct keys
    $x \neq y$.
    Let $t$ be the special index given by Sign Near Independence.
    Using \cref{eq:near-independence} twice we get that
    \begin{equation}\begin{split}\label{eq:twice-split}
        \E[s_x s_y f_x f_y [i_x = i_y]]
            &= \E[s_x f_x f_y [i_x = i_y] \mid i_y = t]/p
            \\&= \E[s_x f_x f_y [i_x = t]]/p
            \\&= \E[f_x f_y [i_x = t] \mid i_x = t]/p
            \\&= f_x f_y / p^2 \; .
    \end{split}\end{equation}
    From this, we can calculate $\E[X]$.
    \begin{align*}
        \E[X]
            = F_2 + \sum_{x \neq y} \E[s_x s_y f_x f_y [i_x = i_y]]
            = F_2 + (F_1^2 - F_2)/p^2 .
    \end{align*}
    Now we note that $0 \le F_1^2 \le n F_2$ by Cauchy-Schwarz, hence we get that
    $| \E[X] - F_2 | \le (n - 1)/p^2$.

    The same method is applied to the analysis of the variance, which is
    \[
        \Var[X]
            = \Var[Y]
            \le \E[Y^2]
            = \sum_{x,y,x',y' \in [u], x \neq y, x' \neq y'} \E[(s_x s_y f_x f_y [i_x = i_y]) (s_{x'} s_{y'} f_{x'} f_{y'}[i_{x'} = i_{y'}])]
        \; .
    \] 
    Consider any term in the sum. Suppose some key, say $x$, is unique in the
    sense that $x \not \in \{y,x',y'\}$. Then we can apply \cref{eq:near-independence}.
    Given that $x \neq y$ and $x'\neq y'$, we have either $2$ or $4$ such unique keys.
    If all 4 keys are distinct, as in \cref{eq:twice-split}, we get
    \begin{align*}
        \E[(s_x s_y f_x f_y [i_x = i_y]) &(s_{x'} s_{y'} f_{x'} f_{y'}[i_{x'} = i_{y'}])]
            \\&= \E[(s_x s_y f_x f_y [i_x = i_y])] \E[s_{x'} s_{y'} f_{x'} f_{y'}[i_{x'} = i_{y'}])]
            \\&= (f_x f_y/p^2)(f_{x'} f_{y'}/p^2)
            \\&= f_x f_y f_{x'} f_{y'}/p^4
        \; .
    \end{align*}
    The expected sum over all such terms is thus bounded
    as 
    \begin{equation}\begin{split}
        \sum_{{\rm distinct}\, x,y,x',y'\in[u]}& \E[(s_x s_y f_x f_y [i_x = i_y]) (s_{x'} s_{y'} f_{x'} f_{y'}[i_{x'} = i_{y'}])]
            \\&= \sum_{{\rm distinct}\,x,y,x',y'\in[u]} f_xf_yf_{x'}f_{y'}/p^4
            \\&\le F_1^4 /p^4
            \\&\le F_2^2 n^2/p^4.\label{eq:distinct}
    \end{split}\end{equation}
    Where the last inequality used Cauchy-Schwarz. We also have to consider all the cases with
    two unique keys, e.g., $x$ and $x'$ unique while $y=y'$. Then using \cref{eq:near-independence}
    and \cref{eq:prob-special-value}, we get
    \begin{align*}
        \E[(s_x s_y f_x f_y [i_x = i_y]) &(s_{x'} s_{y'} f_{x'} f_{y'}[i_{x'} = i_{y'}])]
            \\&= f_x f_{x'} f_y^2 \E[s_x s_{x'} [i_x = i_{x'} = i_y]]
            \\&= f_x f_{x'} f_y^2 \E[s_{x'} [t = i_{x'} = i_y]]/p
            \\&= f_x f_{x'} f_y^2 \E[t = i_y]/p^2
            \\&\le f_x f_{x'} f_y^2(1 + \delta)/(rp^2).
    \end{align*}    
    Summing over all terms with $x$ and $x'$ unique while $y=y'$, and
    using Cauchy-Schwarz and $u\leq p$, we get 
    \begin{align*}
        \sum_{{\rm distinct}\,x,x',y} f_xf_{x'}f_y^2 (1 + \delta) /(rp^2) 
            \le F_1^2 F_2 (1 + \delta)/(rp^2)
            \le F_2^2 n(1 + \delta)/(rp^2).
    \end{align*}
    There are four ways we can pick the two unique keys $a\in \{x,y\}$
    and $b\in \{x',y'\}$, so we conclude that
    \begin{equation}\label{eq:one-pair}
        \sum_{\substack{
            x,y,x',y'\in[u], x\neq y, x'\neq y',\\
            (x,y)=(x',y')\,\vee\,(x,y)=(y',x')
        }}
        \E[(s_x s_y f_x f_y [i_x = i_y]) (s_{x'} s_{y'} f_{x'} f_{y'}[i_{x'} = i_{y'}])]
            \le 4 F_2^2 n(1 + \delta)/(rp^2) .
    \end{equation}
    Finally, we need to reconsider the terms with two pairs, that
    is where $(x,y)=(x',y')$ or $(x,y)=(y',x')$. In
    this case, $(s_x s_y f_x f_y [i_x = i_y]) (s_{x'} s_{y'} f_{x'} f_{y'}[i_{x'} = i_{y'}]) = f_x^2 f_y^2 [i_x = i_y]$.
    By \cref{eq:collision}, we get 
    \begin{equation}\begin{split}    
        \sum_{\substack{
            x,y,x',y'\in[u], x\neq y, x'\neq y',\\
            (x,y)=(x',y')\,\vee\,(x,y)=(y',x')
        }}&
            \E[(s_x s_y f_x f_y [i_x = i_y]) (s_{x'} s_{y'} f_{x'} f_{y'}[i_{x'} = i_{y'}])]
            \\&=2\sum_{x,y\in[u],x\neq y} f_x^2f_y^2 \Pr[i_x=i_y]
            \\&=2\sum_{x,y\in[u],x\neq y} f_x^2f_y^2 (1 + \eps)/r
            \\&=2(F_2^2 - F_4)(1 + \eps)/r .\label{eq:two-pairs}
    \end{split}\end{equation}
    Adding up add \eqref{eq:distinct}, \eqref{eq:one-pair}, and
    \eqref{eq:two-pairs}, we get 
    \begin{align*}
        \Var[Y]
            &\le 2(1 + \eps)(F_2^2 - F_4)/r + F_2^2(4(1 + \delta) n / (rp^2) + n^2/p^4)
            \\&\le 2F_2^2/r + F_2^2 (2\eps/r + 4(1 + \delta)n / (rp^2) + n^2/p^4 - 2 /(rn)) .
    \end{align*}
    This finishes the proof.
\end{proof}

We are now ready to prove \Cref{thm:h-and-s-p}.
\begingroup
    \def\thelemma{\ref{thm:h-and-s-p}}
    \begin{theorem}
        Let $r>1$ and $u>r$ be powers of two and let $p=2^b-1>u$ be a
        Mersenne prime.
        Suppose we have a 4-universal hash function $h:[u]\to[2^b-1]$, e.g.,
        generated using Algorithm \ref{alg:Mersenne}. Suppose
        $i:[u]\to[r]$ and
        $s:[u]\to\{-1,1\}$ are constructed from $h$ as described in
        Algorithm \ref{alg:h-and-s}. Using this $i$ and $s$ 
        in the Count Sketch Algorithm \ref{alg:count-sketch}, the second moment 
        estimate $X=\sum_{i\in[k]} C_i^2$ satisfies:
        \begin{align}
           \E[X] &= F_2+(F_1^2-F_2)/p^2, \label{eq:E-F2-p}\\
           | \E[X] - F_2 | &\le F_2 (n - 1)/p^2, \label{eq:E-F2-p-com}\\
           \Var[X]&< 2F_2^2/r.\label{eq:V-F2-p}
        \end{align}
    \end{theorem}
    \addtocounter{lemma}{-1}
\endgroup

From \Cref{eq:remove-si} and \Cref{eq:coll-ell=r-1} we have
\nameref{prop:near-independence} with $
    \Pr[i_x = 2^b - 1] \le (1 - (r - 1)/p)/r$
   and \Cref{eq:coll} $(1 + (r - 1)/p^2)/r$-low collision probability
% that \Cref{prop:near-independence} is satisfied with $t = r - 1$
% and $\delta = -(r - 1)/p$, and \Cref{eq:coll} implies that
% \Cref{prop:collision} is satisfied with $\eps = (r - 1)/p^2$.
Now \Cref{lem:count-mersenne} give us \eqref{eq:E-F2-p}
and \eqref{eq:E-F2-p-com}. Furthermore, we have that
\begin{align*}
    \Var[X] 
        &\le 2F_2^2/r + F_2^2 (2\eps/r + 4(1 + \delta)n / (rp^2) + n^2/p^4 - 2 /(rn))
        \\&= 2F_2^2/r + F_2^2(2/p^2 + 4n/(r p^2) + n^2/p^4 - 2/(rn)) .
\end{align*}

We know that $2 \le r \le u/2 \le (p + 1)/4$ and $n \le u$.
This implies that $p \ge 7$ and that $n/p \le u/p \le 4/7$.
We want to prove that
$2/p^2 + 4n/(r p^2) + n^2/p^4 - 2/(rn) \le 0$ which would
prove our result. We get that
\begin{align*}
    2/p^2 + 4n/(r p^2) + n^2/p^4 - 2/(rn)
        \le 2/p^2 + 4u/(r p^2) + u^2/p^4 - 2/(ru) .
\end{align*}
Now we note that $4u/(r p^2) - 2/(ru) = (2u^2 - p^2)/(u p^2 r) \le 0$
since $u \le (p + 1)/2$ so it maximized when $r = u/2$. We then get
that
\begin{align*}
    2/p^2 + 4u/(r p^2) + u^2/p^4 - 2/(ru)
        \le 2/p^2 + 8/p^2 + u^2 / p^4 - 4/u^2 .
\end{align*}
We now use that $u/p \le (4/7)^2$ and get that
\begin{align*}
    2/p^2 + 8/p^2 + u^2 / p^4 - 4/u^2
        \le (10 + (4/7)^2 - 4 (7/4)^2)/p^2
        \le 0 .
\end{align*}
This finishes the proof of \eqref{eq:V-F2-p} and thus also of \Cref{thm:h-and-s-p}.
