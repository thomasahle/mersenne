\documentclass[12pt]{article}  
%\renewcommand\baselinestretch{0.95}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{times}
\usepackage{fullpage}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{todonotes}
\usepackage{hyperref}
\usepackage{algorithmic}
\usepackage[ruled,vlined,commentsnumbered,titlenotnumbered]{algorithm2e}
\newcommand{\suchthat}{\mathrel{}\mathclose{}\ifnum\currentgrouptype=16\middle\fi\vert\mathopen{}\mathrel{}}
\newcommand{\xor}{\oplus}
\newcommand{\ls}{\texttt{<<}}
\newcommand{\rs}{\texttt{>>}\,}
\newcommand{\msb}{{\sf msb}}
%\newcommand{\E}{{\sf E}}
\DeclareMathOperator*{\E}{\sf{E}}
\newcommand{\Var}{{\sf Var}}
\newcommand{\ttl}{{\tt 1}}
\newcommand{\tto}{{\tt 0}}
\newcommand{\estr}{\backslash 0}

\newcommand{\drop}[1]{}
\newcommand{\ppmod}{\rule{-1.5ex}{0ex}\pmod}
\newcommand{\lpf}{\textnormal{lin-prob-fill}}
\newcommand{\length}{\textnormal{length}}
\newcommand{\case}[1]{\underline{{#1}:}}
\newcommand{\floor}[1]{\lfloor {#1} \rfloor}
\newcommand{\Prp}[1]{\Pr\left[{#1} \right]}
\newcommand{\Ep}[1]{{\sf E}\left[{#1} \right]}
\newcommand{\req}[1]{(\ref{#1})}
\newcommand{\ceiling}[1]{\lceil {#1} \rceil}
\newcommand{\bigfloor}[1]{\left\lfloor {#1} \right\rfloor}
\newcommand{\bigceiling}[1]{\left\lceil {#1} \right\rceil}
\newtheorem {invariant} {Invariant}
\renewcommand\theinvariant{\Roman{invariant}}
\newtheorem {lemma} {Lemma}[section]
\newtheorem {fact} [lemma] {Fact}
\newtheorem {definition} {Definition}
\newtheorem {corollary} [lemma] {Corollary}
\newtheorem {theorem}[lemma] {Theorem}
\newtheorem {observation}[lemma] {Observation}
\newtheorem {question}{Question}[section]
\newtheorem {exercise}[question]{Exercise}
\newcommand{\qed}{\rule{1ex}{1ex}}
\newenvironment{proof}[1][]{\paragraph*{Proof{#1}}}{\hfill \qed\smallskip\\}
\newcommand\cH{{\mathcal H}}
\newcommand\eps\varepsilon
\newcommand\fct\rightarrow
\newcommand\asgn\leftarrow
\newcommand\baralpha{(1-\alpha)}
\newcommand\ceil[1]{\lceil {#1}\rceil}
\newcommand\hshift{h_a^{\gg}}
\newcommand\ol\overline
\newcommand\Z{\mathbb Z}

% Eva's comments in a different colour
\usepackage{color}
\usepackage{xcolor}
\newcommand{\er}[1]{\textcolor{blue}{#1}}
\newcommand{\erdel}[1]{\textcolor{LightGreen}{#1}}


\title{Fast second moment estimation and other fun with Mersenne primes} 
\author{Mikkel Thorup}
\begin{document}
\maketitle 

\begin{abstract}
The classic way of computing a $k$-universal hash function is
to use a random degree-$(k-1)$ polynomial over a prime field $\mathbb Z_p$.
For a fast computation of the polynomial, the prime $p$ is often
chosen as a Mersenne prime $p=2^b-1$.

In this paper, we show that there are other nice advantages to using
Mersenne primes. Our view is that the output of the hash function is a
$b$-bit integer that is uniformly distributed in $[2^b]$, except that
$p$ (the all \texttt1s value) is missing. Thinking of the hash
values as almost uniform $b$-bit integers leads to simple efficient code 
with strong theoretical qualities. We will demonstrate this with focus on the
4-universal hashing in the classic count-sketch for second moment
estimation.
\end{abstract}

\section{Introduction}
Polyomial hashing using Mersenne primes is well-known for being faster
than polynomial hashing using arbitrary primes. Here we argue that
uniform hash values from a Mersenne prime field with prime $p=2^b-1$
can largely be treated as uniform $b$-bit strings, that is, we can use
the tool box of very simple and efficient tricks for uniform
$b$-bit strings.

From $[2^b]$ we are missing $p$, the all \texttt1s value, but
a careful analysis shows that this has only minor effect
on the quality of the outcome. Many of the ideas presented
here would not apply at all if we had a prime $2^b-a$ with $a>1$, e.g.,
$a=3$, so our work is very particular to Mersenne primes.

To put our results in perspective, suppose we were hashing $n$ keys uniformly
into $b$-bit strings. The probability that any of them hash
to $p=2^b-1$ is at most $n/p$. This means that any error
probability we might have proved assuming uniform $b$-bit hash
values is off by at most $n/p$. This may be good if $p/n$ is
sufficiently large. However, the analysis we present yields 
errors that below $n/p^2$ which is good if $p$ is large
even if $n$ is close to $p$.

\subsection{Polynomial hashing using Mersenne primes}
The classic definition of $k$-universal (or $k$-independent) hashing
goes back to Carter and Wegman \cite{wegman81kwise}.
\begin{definition}
A random hash function $h:U\fct R$ is $k$-universal if for any $j\leq k$
distinct keys $x_0,\ldots,x_{j-1}$, the vector
$(h(x_0),\ldots,h(x_{j-1}))$ is uniformly distributed in $R^j$. This
is equivalent to saying that each $h(x_i)$ is uniform in $R$ and that
all the $h(x_i)$ are indepent of each other.
\end{definition}
We generally use the notation $[s]=\{0,\ldots,s-1\}$.
The classic example of $k$-universal
hash function is uniformly random degree-$(k-1)$ polynomial over a prime field
$\Z_p$, that is, we pick a uniformly random vector
$\vec a=(a_0,\ldots,a_{k-1})\in \Z_p^k$ of $k$ coefficients, and define
$h_{\vec a}:[p]\fct[p]$, by 
\[h_{\vec a}(x)=\left(\sum_{x\in[k]}a_i x^i\right)\bmod p.\]

Often we are given a key domain $[u]$ and a range of hash values $[r]$. Assuming $p\geq \max\{u,r\}$, we define
$h^r_{\vec a}:[u]\fct[r]$, by
\[h^r_{\vec a}(x)=h_{\vec a}(x)\bmod r.\]
With this definition, the hash values of $k$ distinct keys 
remain independent, but they are not exactly uniform. More
precisely, for any $i\in[r]$, we have that
$\Pr[h_{\vec a}^r(x)=i]$ is
either $\lfloor p/r\rfloor/p$ or $\lceil p/r\rceil/p$. In
either case, this is $(1\pm r/p)/r$, so this is close to uniform if
$p\gg r$.


The main problem with the above approach is speed because computing
the mod-function is slow on most computers. Often it
is OK if we for the hash range $[r]$ pick $r=2^\ell$ as a power of two.
Then $r-1$ consists of $\ell$ \texttt1s, and then 
\[y\bmod r=y\,\texttt\&\,(r-1)\]
where \texttt\& denotes bit-wise {\sc and}. Bit-wise operations are
extremely fast, so now $\bmod\,r$ is not an issue.

For general primes, computing $\bmod\,p$ is slow, but an old idea,
mentioned by Carter and Wegmen \cite{carter77universal}, is to use a
Mersenne prime $p=2^b-1$, e.g., $p=2^{61}-1$ for hashing 32-bit keys or
$p=2^{89}-1$ for hashing 64-bit keys. The point in using such a Mersenne prime
is that
\begin{equation}\label{eq:Mersenne}
y\equiv y\bmod 2^{b}+\floor{y/2^b}\ppmod {p}.
\end{equation}
This leads to efficient computations because 
\[y\bmod 2^{b}=y\texttt{\&}p\quad\textnormal{and}\quad\floor{y/2^b}=y\texttt{>>}b.\]
Here the bit-wise {\sc and} (\texttt{\&}) and the right-shift (\texttt{>>}) are
both very fast operations. For
an extra fast implementation, we further assume that $p=2^b-1\geq 2u-1$.
This is automatically satisfied in the typical case where $u$ is a power
of two, e.g., $2^{32}$ or $2^{64}$. This is all used in Algorithm \ref{alg:Mersenne} to make a very fast
computation of a degree $(k-1)$-polynomial using Horner's rule. Note that
the reduction $y\gets (y\texttt\&p)+(y\texttt{>>}b)$ is applied after
every multiplication so that the numbers involved never get too big (multiplication of large numbers is expensive).
\begin{algorithm}\label{alg:Mersenne}
  \caption{For $x\in [u]$, prime $p=2^b-1\geq 2u-1$, and
    $\vec a=(a_0,\ldots,a_{k-1})\in[p]^k$, compute $h_{\vec a}(x)=\left(\sum_{i\in[q]}a_i x^i\right)\bmod p$}
$y\gets a_{k-1}$\;
  \For{$i=q-2,\ldots,0$}{
    \ \\[-4ex]
    \hfill $\rhd\quad y<2p$\\
$y\gets yx+a_i$\hfill $\rhd\quad y<2p(u-1)+(p-1)<(2u-1)p\leq p^2$\\
$y\gets (y\texttt\&p)+(y\texttt{>>}b)$ \hfill $\rhd\quad y<p+p^2/2^b<2p$}
%  $y\gets y+1$ \hfill $\rhd\quad 0<y\leq 2p$, \ $(y\texttt{>>}b)\leq 1$, and
%  $y\geq 2^b\implies y\texttt\&p=y-2^b<p$\\
%$y\gets (y\texttt\&p)+(y\texttt{>>}b)$ \hfill $\rhd\quad 0<y\leq p$\\
%$y\gets y-1$\hfill $\rhd\quad y\in[p]$\\
\lIf{$y\geq p$}{$y\gets y-p$}
{\bf return} $y$
\end{algorithm}
The above completes our description of how Mersenne primes are
normally used for fast computation of $k$-universal hash functions.
We shall in fact show how it can be made even faster, but the main
point in this paper is to present an analysis, showing how our
$k$-universal hash values from $[2^b-1]$ can be almost as good as if
they were uniformly distributed $b$-bit strings (we are only missing
the all \texttt{1}s value $2^b-1$). 

We note that $k=2$, we do have the fast multiply-shift scheme of Dietzfelbinger
\cite{dietzfel96universal}, that directly gives 2-universal
hashing from $b$-bit strings to $\ell$-bit strings, but for $k>2$,
there is no faster method that can be implemented in with portable code
in a standard programming language like C.




\subsubsection{Good bucketting with powers of two}\label{sec:power-of-two}
As a first trivial illustration of the quality that we get using a
Mersenne prime $p=2^b-1$, consider the case mentioned above where we
want hash values in the range $[r]$ where $r=2^\ell$ is a power of
two. We will often refer to the hash values in $[r]$ as buckets so
that we are hashing keys to buckets. Avoiding a degenerate case, we
assume $r>1$. In particular this implies that $r$ does not divide our
prime $p$.

We assume a $k$-universal hash function $h:[u]\fct[p]$, e.g.,
the one from Algorithm \ref{alg:Mersenne}. To get a hash values in $[r]$,
we defined $h^r:[u]\fct[r]$ by
\[h^r(x)=h(x)\bmod r=h(x)\texttt \& (r-1).\]
As discussed previously, the hash values of up to $k$ distinct keys remain
independent with $h^r$. The issue is that hash values from 
$h^r$ are not quite uniform in $[r]$.

Recall that for any key $x$, we have $h(x)$ uniformly distributed in $[2^b-1]$.
This is the uniform distribution on $b$-bit strings except that we are
missing $p=2^b-1$. Now $p$ is the all \texttt{1}s, and 
$p\texttt \& (r-1)=r-1$. Therefore, for any 
any $i<r-1$,
\begin{equation}\label{eq:coll-ell<r-1}
\Pr[h^r(x)=i]=\lceil p/r\rceil/p=((p+1)/r)/p
=(1+1/p)/r\textnormal,
\end{equation}
while 
\begin{equation}\label{eq:coll-ell=r-1}
 \Pr[h^r(x)=r-1]=\lfloor p/r\rfloor/p=((p+1-r)/r)/p
=(1-(r-1)/p)/r.
\end{equation}
Thus $\Pr[h^r(x)=i]\leq (1+1/p)/r$ for all $i\in[r]$. This upper-bound
only has a relative error of $1/p$ from the uniform $1/r$. For
comparison, if we had used a prime of the form $p=2^b-a$ and $a<r$, then
we would only get an upper bound of $(1+a/p)/r$. Below we return
to a Mersenne prime $p=2^b-1$

Combining \req{eq:coll-ell<r-1} and \req{eq:coll-ell=r-1} with
pairwise independence, for any distinct keys $x,y\in [u]$, we get
collision probability
\begin{align}
  \Pr[h^r(x)=h^r(y)]&=\sum_{i\in[r]}\Pr[h^r(x)=h^r(y)=i]=\sum_{i\in[r]}\Pr[h^r(x)=i]^2\nonumber\\
                   &=(r-1)((1+1/p)/r)^2+((1-(r-1)r/p)/r)^2\nonumber\\[.5ex]
  &=\frac{r +(r^2-r)/p^2}{r^2}=(1+(r-1)/p^2)/r<(1+r/p^2)/r.\label{eq:coll}
  \end{align}
We note that relative error $r/p^2$ is small as long as $p$ is
large.

\paragraph{Selecting arbitrary bits from the hash value}
Interestingly, the above analysis holds no matter which $\ell$ bits we
use when mapping the hash values from $[2^b-1]$ to $[2^\ell]$.  Let
$\mu:[2^b]\fct[2^\ell]$ be any map defined by selecting $\ell$ bits
from a $b$-bit string. Above we used
$\mu(y)=y\bmod 2^\ell=y\texttt\& (2^\ell-1)$,
selecting the $\ell$ least significant bits, but we could
also use $\mu(y)=\floor{y/2^{b-\ell}}=y\texttt{>>}(b-\ell)$ selecting
the $\ell$ most significant bits. The basic point is that a uniform
distribution on $[2^b]$ maps to a uniform distribution on
$[2^\ell]$. We are only missing the all \texttt1s value $p=2^b-1$ which maps to $2^\ell-1$
regardless of which $\ell$ bits we select, so our equations
\req{eq:coll-ell<r-1}--\req{eq:coll} hold no matter which $\ell$
bits we select for $h^r$.

The fact that it doesn't matter which $\ell$ bits we select is only
true because we use a Mersenne prime $p=2^b-1$. Suppose we used some
other $b$-bit prime $p=2^b-a$ where $2^{b-\ell}<a<2^{b-1}$. If we
select the $\ell$ most signifant bits, then $0$ elements from $[p]$
map to $2^\ell-1$ while $2^{b-\ell}$ elements from $[p]$ map to $0$. However,
with the $\ell$ least significant bits, we have $\floor{p/2^\ell}$ or
$\ceil{p/2^\ell}$ elements from $[p]$ mapping to each element in
$[2^\ell]$, so the maximal difference is 1.





\subsection{Two-for-one hash functions in second moment estimation}
In this section we discuss how we can get several hash functions for
the price of one, and apply the idea to second moment estimation using
count sketches \cite{charikar04count-sketch}.

Suppose we had a $k$-universal hash function into $b$-bit strings.
We note that using standard programming languages such as C, we have
no simple and efficient method computing such hash
functions when $k>2$. However, later we will argue that polynomial
hashing using a Mersenne prime $2^b-1$ delivers a better-than-expected
approximation.

Let $h:U\fct [2^b]$ be $k$-universal. By definition this
means that if we have $j\leq k$ distinct keys $x_1,\ldots,x_j$, then
$(h(x_1),\ldots,h(x_j))$ is uniform in $[2^b]^j\equiv [2]^{bj}$,
so this means that \emph{all} the bits in $h(x_1),\ldots,h(x_j)$ are
independent and uniform. We can use this to split our $b$-bit hash
values into smaller segments, and sometimes use them as if
they were the output of independently computed hash functions.
We illustrate this idea below in the context of the second moment estimation.

\subsubsection{Second moment estimation}
We now review the second moment estimation of streams based on count
sketches \cite{charikar04count-sketch} (which are based on the
celebrated second moment AMS-estimator from \cite{alon96frequency})

The basic set-up is as follows.  For keys in $[u]$ and integer values in $\Z$, we are given a stream of key/value $(x_0,\Delta_0),\ldots, (x_{n-1},\Delta_{n-1})\in [u]\times\Z$. The
total value of key $x\in[u]$ is
\[f_x=\sum_{i\in[n],x_i=x} \Delta_i.\]
We let $n\leq u$ be  the number of non-zero values
$f_x\neq 0$, $x\in [u]$. Often $n$ is much smaller than $u$.
We define the $m$th moment
$F_m=\sum_{x\in [u]}f_y^m=\|f\|_m^m$. The goal here is to
estimate the second moment $F_2=\sum_{x\in [u]}f_x^2$ which is the square of the Euclidean norm $\|f\|_2$. 

\begin{algorithm}
  \caption{\label{alg:count-sketch} Count Sketch. Uses a
vector/array $C$ of $r$ integers and two independent
4-universal hash functions $i:[u]\fct[r]$ and $s:[u]\fct\{-1,1\}$.
.}
\begin{description}
\item[Initialize] For $i\in[t]$, set $C[i]\asgn 0$.
\item[Process$(x,\Delta)$] $C[i(x)]\asgn C[i(x)]+s(x) \Delta$. 
\item[Output] $X=\sum_{i\in[t]} C[i]^2$.
\end{description}
\end{algorithm}
The standard analysis \cite{charikar04count-sketch} shows that 
\begin{align}
\E[X]&=F_2\label{eq:E-F2}\\
\Var[X]&=2(F_2-F_4)/r<2F_2/r\label{eq:V-F2}
\end{align}
By Chebyshev's inequality, this implies
\[\Pr[|X-F_2|\geq \eps F_2]\leq \Var[X]/(\eps F_2)^2<
2/(k\eps^2).\]
With $t=8/\eps^2$, the error probability is below 1/4.
To
reduce the error probability, we can use the standard trick of
making $r$ independent experiments
and return the median estimate. Using Chernoff bounds, the probability
that the median deviates by more than $\eps F_2$ is bounded by
$\exp(-r/12)$.

\subsubsection{Two-for-one hash functions with $b$-bit hash values}
As the count sketch is described above,
it uses two independent 4-universal hash functions
$i:[u]\fct[r]$ and $s:[u]\fct\{-1,1\}$, but 4-universal hash functions
are generally slow to compute, so, aiming to save roughly a factor 2
in speed, a tempting idea is to compute them both using a single hash
function.

The analysis behind \req{eq:E-F2} and \req{eq:V-F2} does not quite
require $i:[u]\fct[r]$ and $s:[u]\fct\{-1,1\}$ to be indepedent.
It suffices that the hash values are uniform and that for any
given set of $j\leq 4$ distinct keys $x_1,\ldots,x_j$, the $2j$ hash
values $i(x_1),\ldots,i(x_j),s(x_1),\ldots,s(x_j)$ are independent.
A critical step in the analysis is that if
$A$ depends on $i(x_1),\ldots,i(x_j),s(x_2),\ldots,s(x_j)$, but
not on $s(x_1)$, then
\[\E[s(x_1)A]=0.\]
This follows because $\E[s(x_1)]=0$ by uniformity of $s(x_1)$ and because $s(x_1)$ is independent of $A$.
  

Assuming that $t=2^\ell$ is a power of two, we can easily construct
$i:[u]\fct[t]$ and $s:[u]\fct\{-1,1\}$ using a single $4$-universal
hash function $h:[u]\fct[2^b]$ where $b>\ell$. Recall that all the bits in
$h(x_1),\ldots,h(x_4)$ are independent. We can therefore use the
$\ell$ least significant bits of $h(x)$ for $i(x)$ and the most
significant bit of $h(x)$ for a bit $a(x)\in[2]$, and finally set
$s(x)=1-2a(x)$. The construction is summarized in Algorithm \ref{alg:h-and-s}
\begin{algorithm}\label{alg:h-and-s}
  \caption{For key $x\in [u]$, compute $i(x)=i_x\in[2^\ell]$ and
    $s(x)=s_x\in\{-1,+1\}$,\rule{5ex}{0ex}
    using $h:[u]\fct [2^b]$ where $b>\ell$.}
  $h_x\gets h(x)$\hfill $\rhd\quad h_x$ uses $b$ bits\\
  $i_x\gets h_x\&(2^\ell-1)$\hfill $\rhd\quad i_x$ gets $\ell$ least significant
    bits of $h_x$\\
$a_x\gets h_x\texttt{>>}(b-1)$\hfill $\rhd\quad a_x$ gets the most significant bit of $h_x$\\
$s_x\gets 1-2a_x$ \hfill $\rhd\quad a_x\in[2]$ is converted to a sign $s_x\in\{-1,+1\}$
\end{algorithm}
\begin{lemma}\label{lem:b-bit-hashing} Suppose $h:[u]\fct[2^b]$ is $k$-universal. Let
  $i:[u]\fct[2^\ell]$ and
  $s:[u]\fct\{-1,1\}$ be constructed from $h$ as described in Algorithm \ref{alg:h-and-s}. Then $h$ and $s$ are both $k$-universal. Moreover, for
  any $j\leq k$ distinct keys $x_1,\ldots,x_j$, the $2j$ hash
  values $i(x_1),\ldots,i(x_j),s(x_1),\ldots,s(x_j)$ are independent.
  In particular, if $A$ depends on
  $i(x_1),\ldots,i(x_j),s(x_2),\ldots,s(x_j)$, but not on $s(x_1)$, then
\begin{equation}\label{eq:E-0}
  \E[s(x_1)A]=0
\end{equation}
\end{lemma}
Note that Algorithm \ref{alg:h-and-s} is well defined as long as 
$h$ returns a $b$-bit integer. However, Lemma \ref{lem:b-bit-hashing} requires
that $h$ is $k$-universal into $[2^b]$, which in particular implies that
the hash values are uniform in $[2^b]$.


\subsubsection{Two-for-one hashing with  Mersenne primes}\label{sec:two-for-one}
Above we discussed how useful it would be with $k$-universal hashing
mapping uniformly into $b$-bit strings. The issue was that the lack of
efficient implementations with standard portable code if
$k>2$. However, when $2^b-1$ is a Mersenne prime $p\geq u$, then we do
have have the efficient computation from Algorithm \ref{alg:Mersenne}
of a $k$-universal hash function $h:[u]\fct[2^b-1]$. The hash values
are $b$-bit integers, and they are uniformly distributed, except that
we are missing the all \texttt{1}s value $p=2^b-1$. We want to
understand how this missing value affects us if we try to split the
hash values as in Algorithm \ref{alg:h-and-s}. Thus, we assume a
$k$-universal hash function $h:[u]\fct[2^b-1]$ from which we construct
$i:[u]\fct[2^\ell]$ and $s:[u]\fct\{-1,1\}$ as
described in Algorithm \ref{alg:h-and-s}. As usual, we assume $2^\ell>1$.
Since $i_x$ and $s_x$ are
both obtained by selection of bits from $h_x$, we know from Section
\ref{sec:power-of-two} that each of them have close to uniform
distributions. However, we need a good replacement for \req{eq:E-0}
which besides uniformity, requires $i_x$ and $s_x$ to be independent,
and this is certainly not the case.

Before getting into the analysis, we argue that we really do get two
hash functions for the price of one. The point is that our efficient
computation in Algorithm \ref{alg:Mersenne} requires that we use a
Mersenne prime $2^b-1$ such that $u\leq 2^{b-1}$, and this is even if
our final target is to produce just a single bit for the sign function
$s:[u]\fct\{-1,1\}$. We also know that $2^\ell<u$, for otherwise we
get perfect results implementing $i:[u]\fct[2^\ell]$ as the identifty
function (perfect because it is collision free).  Thus we can assume
$\ell<b$, hence that $h$ provides enough bits for both $s$ and $i$.


We now consider the effect of the hash values from $h$ being uniform
in $[2^b-1]$ instead of in $[2^b]$. Suppose we want to compute the
expected value of an expression $B$ depending only on the independent
hash values $h(x_1),\ldots,h(x_j)$ of $j\leq k$ distinct keys
$x_1,\ldots,x_j$.

Our generic idea is to play with the distribution of $h(x_1)$ while
leaving the distributions of the other independent hash values
$h(x_2)\ldots,h(x_j)$ unchanged, that is, they remain uniform in
$[2^b-1]$. We will consider having $h(x_1)$ uniformly distributed in
$[2^b]$, denoted $h(x_1)\gets U[2^b]$, but then we later have to
subtract the ``fake'' case where $h(x_1)=p=2^b-1$.  Making the
distribution of $h(x_1)$ explicit, we get
  \begin{align}
  \E_{h(x_1)\gets U[p]}[B]&=\sum_{y\in[p]}\E_{h(x_1)=y}[B]/p
  =\sum_{y\in[2^b]}\E_{h(x_1)=y}[B]/p-\E_{h(x_1)=p}[B]/p\nonumber\\
  &=\E_{h(x_1)\gets U[2^b]}[B](p+1)/p-\E_{h(x_1)=p}[B]/p.\label{eq:play-with-dist}
\end{align}
Let us now apply this idea our situation where $i:[u]\fct[2^\ell]$ and
$s:[u]\fct\{-1,1\}$ are constructed from $h$ as described in Algorithm
\ref{alg:h-and-s}. We will prove
\begin{lemma}\label{lem:remove-si}  Consider distinct keys $x_1,\ldots,x_j$, $j\leq k$ and an expression $B=s(x_1)A$ where $A$
depends on $i(x_1),\ldots,i(x_j)$ and $s(x_2),\ldots,s(x_j)$ but not
$s(x_1)$. Then
\begin{equation}\label{eq:remove-si}
  \E[s(x_1)A]=\E[A\mid i(x_1)=2^\ell-1]/p.
\end{equation}
\end{lemma}
\begin{proof}
When $h(x_1)\gets U[2^b]$, then $s(x_1)$ is uniform
in $\{-1,+1\}$ and independent of $i(x_1)$. The remaining
$(i(x_i),s(x_i))$, $i>1$, are independent of $s(x_1)$ because they
are functions of $h(x_i)$ which is independent of $h(x_1)$, so
we conclude that 
\[\E_{h(x)\gets U[p+1]}[s(x_1)A]=0\]
Finally, when $h(x_1)=p$, we get $s(x_1)=-1$ and $i(x_1)=2^\ell-1$, 
so applying \req{eq:play-with-dist}, we conclude
that 
\[\E[s(x_1)A]=\E_{i(x)=2^\ell-1}[A]/p.\]
\end{proof}
Above \req{eq:remove-si} is our replacement for \req{eq:E-0}, that is,
when the hash values from $h$ are uniform in $[2^b-1]$ instead of
in $[2^b]$, then $\E[s(x_1)B]$ is reduced by $\E_{i(x)=2^\ell-1}[B]/p$.
For large $p$, this is a small additive error. Using this in a careful
analysis, we will show that our fast second moment estimation 
based on Mersenne primes performes almost perfectly:

\begin{theorem}\label{thm:h-and-s-p}
Let $r>1$ and $u>r$ be powers of two and let $p=2^b-1>u$ be a
Mersenne prime.
Suppose with have a $k$-universal hash function $h:[u]\fct[2^b-1]$, e.g.,
generated using Algorithm \ref{alg:Mersenne}. Suppose
$i:[u]\fct[r]$ and
$s:[u]\fct\{-1,1\}$ are constructed from $h$ as described in
Algorithm \ref{alg:h-and-s}. Using this $i$ and $s$ 
in the Count Sketch Algorithm \ref{alg:count-sketch}, the second moment 
estimate $X=\sum_{i\in[k]} C_i^2$ satsifies:
\begin{align}
\E[X]=&F_2+(F_1^2-F_2)/p^2 < (1+n/p^2)\,F_2\textnormal,\label{eq:E-F2-p}\\
\Var[X]&< 2(F_2^2-F_4)/r+F_2^2 (2.33+4 n/r)/p^2<2F_2^2/r.\label{eq:V-F2-p}
\end{align}
\end{theorem}
The difference from \req{eq:E-F2} and \req{eq:V-F2} 
is negligiable when $p$ is large. Theorem \ref{thm:h-and-s-p} will be
proved in Section \ref{sec:analysis-two-for-one}.


\subsection{Arbitrary number of buckets}\label{sec:most-uniform}
We now consider the general case where we want to hash into a set of
buckets $R$ whose size is not a power of two. Suppose we have a
$2$-universal hash function $h:U\fct Q$. We will compose $h$ with a
map $\mu:Q\fct R$, and use $\mu\circ h$ as a hash function from
$U$ to $R$.

Let $q=|Q|$ and $r=|R|$. We want the map $\mu$ to be \emph{most
  uniform} in the sense that for bucket $i\in R$, the number of
elements from $Q$ mapping to $i$ is either $\floor{q/r}$ or
$\ceil{q/r}$. Then the uniformity of hash values with $h$ implies for
any key $x$ and bucket $i\in R$
\[\floor{q/r}/q\leq \Pr[\mu\circ
  h(x)=i]\leq \ceil{q/r}/q.\]
Below we typically have $Q=[q]$ and $R=[r]$. A standard example of a most uniform map $\mu:[q]\fct[r]$ 
is $\mu(x)=x\bmod r$ which the one used above when we defined 
$h^r:[u]\fct[r]$, but as we mentioned before, the modulo operation is 
quite slow unless $r$ is a power of two.

Another example of a most uniform map $\mu:[q]\fct[r]$ 
is $\mu(x)=\floor{xr/q}$,
which is also quite slow in general, but if $q=2^b$ is a power of two,
it can be implemented as $\mu(x)=(xr)\texttt{>>}\,b$ where 
$\texttt{>>}$ denotes right-shift. This would be yet another advantage 
of of having $k$-universal hashing into $[2^b]$.

Now, our interest is the case where $q$ is a Mersenne prime $p=2^b-1$. We want
an efficient and most uniform map $\mu:[2^b-1]$ into any given $[r]$.
Our simple solution is to define
\begin{equation}\label{eq:most-uniform}
\mu(x)=\floor{(x+1)r/2^b}=((x+1)r)\texttt{>>} b.
\end{equation}
Lemma \ref{lem:most-uniform} (iii) below 
states that \req{eq:most-uniform} indeed
gives a most uniform map. 
\begin{lemma}\label{lem:most-uniform} Let $r$ and $b$ the positive integers
$2^b-1$. 
\begin{itemize}
\item[(i)] $x\mapsto (xr)\texttt{>>}\,b$ is a most
uniform map from $[2^b]$ to $[r]$.
\item[(ii)] $x\mapsto (xr)\texttt{>>}\,b$ is a most
uniform map from $[2^b]\setminus\{0\}=\{1,\ldots,2^b-1\}$ to $[r]$.
\item[(iii)] $x\mapsto ((x+1)r)\texttt{>>}\, b$ is a most
uniform map from $[2^b-1]$ to $[r]$.
\end{itemize}
\end{lemma}
\begin{proof}
Trivially (ii) implies (iii). 
The statement (i) is folklore and easy to prove, so we know that every
$i\in[r]$ gets hit by $\floor {2^b/r}$ or $\ceil{2^b/r}$ elements from
$[2^b]$. It is also clear that $\ceil{2^b/r}$ elements, including $0$,
maps to $0$. To prove (ii), we remove $0$ from $[2^b]$, 
implying that only
$\ceil{2^b/r}-1$ elements map to $0$. For all positive integers $q$
and $r$, $\ceil{(q+1)/r}-1=\floor{q/r}$, and we use this here with 
$q=2^b-1$. It follows that all buckets from $[r]$ gets $\floor{q/r}$
or $\floor{q/r}+1$ elements from $Q=\{1,\ldots,q\}$. If $r$ does
not divide $q$ then $\floor{q/r}+1=\ceil{q/r}$, as desired. However,
if $r$ divides $q$, then $\floor{q/r}=q/r$, and this
is the least number of elements from $Q$ hitting any bucket in $[r]$. Then 
no bucket from $[r]$ can get hit by more than $q/r=\ceil{q/r}$ 
elements from $Q$. This completes the proof of (ii), and hence of (iii).
\end{proof}
We note that our trick does not work when $q=2^b-a$ for $a\geq 2$, that is,
using $x\mapsto ((x+a)r)\texttt{>>} b$, for in this general case, 
the number of elements hashing to $0$ is $\ceil {2^b/r}-a$, or $0$ if
 $a\geq \floor {2^b/r}$. Our new uniform map from \req{eq:most-uniform}
is thus very specific to Mersenne prime fields.

We shall see in Section \ref{sec:count-sketch-r} that our new uniform map
works very well in conjunction with the the idea of splitting of hash values 
values from Section \ref{sec:two-for-one}.




\subsection{Even faster modulo with Mersenne Primes}
We even suggest a speedup the computation of $\bmod p$ for Mersenne primes
$p=2^b-1$. The issue in Algorithm \ref{alg:Mersenne} is that
if-statement can be slow because of branch prediction. 
\begin{algorithm}
  \caption{For Mersenne prime $p=2^b-1$ and $x\leq p^2$, computes
    $y=x\bmod p$ and $x=\floor{x/p}$}
\end{algorithm}
From the standard Algorithm \ref{alg:Mersenne} this means that we can
replaces the last two statements
$y\gets (y\texttt\&p)+(y\texttt{>>}b)$;
if $y\geq p$ then {$y\gets y-p$}





\section{Analysis of second moment estimation using  Mersenne primes}\label{sec:analysis-two-for-one}
In this section, we will prove Theorem \ref{thm:h-and-s-p}.  Thus we
have $r=2^\ell>1$ and $u>r$ both powers of two and $p=2^b-1>u$ a
Mersenne prime.

Now let us set up the relevant variables for the basic count sketch.
For each key $x\in [u]$, we have a value $f_x\in \Z$, and the
goal was to estimate the second moment $F_2=\sum_{x\in u}f^2$.

We had two functions $i:[u]\fct[r]$ and $s:[u]\fct\{-1,+1\}$. 
For notational convinience, we define $i_x=i(x)$ and $s_x=s(x)$.

For each $i\in [r]$, we have a counter 
$C_i=\sum_{x\in[u]} s_x f_x[i_x=i]$, and we define the 
estimator $X=\sum_{i\in[k]} C_i^2$. We want to study how
well it approximates $F_2$.
We have 
\begin{align*}
X&=\sum_{i\in[r]}\left( \sum_{x\in[u]}s_x f_x[i_x=i]\right)^2\\
&=\sum_{i\in[r]}\sum_{x,y\in[u]}s_x f_x[i_x=i=i_y]s_y f_y\\
&=\sum_{x,y\in[u]}s_x f_x[i_x=i_y]s_y f_y.\\
&=\sum_{x\in[u]} f_x^2+\sum_{x,y\in[u],x\neq y}
s_x f_x[i_x=i_y]s_y f_y.
\end{align*}
Thus 
\begin{equation}\label{eq:decomp}
X=F_2+Y\mbox{ where }Y=\sum_{x,y\in[u],x\neq y}
s_x f_x[i_x=i_y]s_y f_y.
\end{equation}
We thus want to provide bounds on the error $Y$.

\subsection{Analysis in the classic case}
First, as a warm-up for later comparison, we analyze the simple classic case
where $i:[u]\fct[2^\ell]$ and
$s:[u]\fct\{-1,1\}$ are independent 4-universal hash
functions. In this case, we first argue that 
\begin{equation}\label{eq:mean}
\E[Y]=0.
\end{equation}
To prove \req{eq:mean}, we argue that each term of
$Y$ from \req{eq:decomp} has 0 expected value. With $x\neq y$, by 2-universality (implied by 4-universality) and independence of $s$ and $h$, we have that $s_x$ is independent of $s_y$, $i_x$, and $i_y$.
Moreover, $\E[s_x]=0$, 
so 
\[\E[s_x f_x[i_x=i_y]s_y f_y]=\E[s_x]\E[f_x[i_x=i_y]s_y f_y]=0.\]
It follows that $\E[Y]=0$, completing the proof of \req{eq:mean}.

Next we want to bound the variance of $X$ which is the same as
that of $Y$ since $F_2$ is a constant. Since
$\E[Y]=0$, we have
\begin{align*}
\Var[X]&=\Var[Y]=\E[Y^2]=\E[(\sum_{x,y\in[u],x\neq y}s_x f_x[i_x=i_y]s_y f_y)^2]\\
&=\sum_{x,y,x',y'\in[u],x\neq y,x'\neq y'}\E[(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})].
\end{align*}
Consider one of the terms $\E[(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]$. We have $x\neq y,x'\neq y'$. 
Suppose that any one of the keys,
say $x$, is unique, that is, $x\not\in\{y,x',y'\}$. Then, by
4-universality, $s_x$ is independent of $s_y$, $s_{x'}$, and $s_{y'}$.
Morever, it is independent of the hash function $h$. Since $\E[s_x]=0$,
we get
\begin{align*}
&\E[(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})] \\
=&\E[s_x]\E[(f_x[i_x=i_y]s_y f_y) 
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]=0.
\end{align*}
Thus we can restict our attention to terms with no unique keys. 
Since $x\neq y$ and $x'\neq y'$, we conclude that 
$(x,y)=(x',y')$ or $(x,y)=(y',x')$. Therefore
\begin{align*}
\Var[X]&=\sum_{x,y,x',y'\in[u],x\neq y,x'\neq y'}\E[(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]\\
       &=2\sum_{x,y\in[u],x\neq y,(x',y')=(x,y)}\E[(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]\\
&=2\sum_{x,y\in[u],x\neq y}\E[(s_x f_x[i_x=i_y]s_y f_y)^2]\\
&=2\sum_{x,y\in[u],x\neq y}\E[(f_x^2f_y^2[i_x=i_y])]\\
&=2\sum_{x,y\in[u],x\neq y}(f_x^2f_y^2)/r\\
&=2(F_2^2-F_4)/r.
\end{align*}
This completes the proof of \req{eq:V-F2}. In the above analysis, we
did not need $s$ and $i$ to be completely independent. All we needed
was that for any $j\leq 4$ distinct keys $x_1,\ldots,x_j$, the hash
values $s(x_1),\ldots,s(x_j)$ and $i(x_1),\ldots,i(x_j)$ are all
independent and uniform in the desired domain. This was why we could
use a single $k$-universal hash function $h:[u]\fct[2^b]$ with
$b>\ell$, and use it to construct $s:[u]\fct\{-1,+1\}$ and
$i:[u]\fct[2^\ell]$ as described in Algorithm \ref{alg:h-and-s}
(c.f. Lemma \ref{lem:b-bit-hashing}).




\subsection{The real analysis of two-for-one using Mersenne primes}
We now consider the case where the functions $s:[u]\fct\{-1,+1\}$ and
$i:[u]\fct[2^\ell]$ are constructed as described in Algorithm
\ref{alg:h-and-s} from a single $k$-universal hash function
$h:[u]\fct[2^b-1]$. Here $h$ could be implemented efficiently as
in Algorithm \ref{alg:Mersenne}. As noted earlier, the function
$i$ is the same as $h^r$ with $r=2^\ell$ in Section \ref{sec:power-of-two},
so it satisfies \req{eq:coll-ell<r-1}--\req{eq:coll}. Also, we
have the power of Lemma \ref{lem:remove-si} to handle the correlation
between $s$ and $i$.

As above, for notational convinience, we let $s_x$ denote $s(x)$ and $i_x$ denote $i(x)$. Also, recall that $n\leq u$ is the number of non-zero values
$f_x\neq 0$, $x\in [u]$.



\paragraph{Expectancy}
Recall from \req{eq:decomp} that
we have
\[X=F_2+Y\mbox{ where }Y=\sum_{x,y\in[u],x\neq y} s_x f_x[i_x=i_y]s_y f_y.\]
This equality holds regardless of random choices, hence regardless of
how $h$ and $s$ are implemented.

To bound $\E[Y]$ we study the expectancy of an arbitrary term, that is,
for some $x,y\in[u],x\neq y$, we study $\E[ s_x f_x[i_x=i_y]s_y f_y]$.
Using Lemma \ref{lem:remove-si}, we get 
\begin{align}
\E[s_x f_x[i_x=i_y]s_y f_y]&=\E[f_x[i_x=i_y]s_y f_y\suchthat i_x=r-1]/p.\nonumber\\
&=\E[s_y f_x[r-1=i_y]f_y]/p.\nonumber\\
&=\E[f_x[r-1=i_y]f_y\suchthat i_y=r-1]/p^2.\nonumber\\
&=\E[f_xf_y]/p^2=f_x f_y/p^2.\label{eq:E}
\end{align}
Thus
\[\E[Y]=\sum_{x,y\in[u],x\neq y} f_xf_y/p^2=(F_1^2-F_2)/p^2.\]
Trivially is the equality from \req{eq:E-F2-p}. For the upper bound, we note that 
\begin{equation}\label{eq:F1F2}
F_1^2\leq nF_2 
\end{equation}
This follows because the ``variance'' over the $|f_x|$ is
$F_2-F_1^2/n=\sum_{x\in[u], f_x\neq 0}(f_x-F_1/n)^2\geq 0$.
Therefore
\[\E[Y]=(F_1^2-F_2)/p^2\leq F_2n/p^2.\]
This completes the proof of \req{eq:E-F2-p}.

\paragraph{Variance}
Same method is applied to the analysis of the variance, which is
\[\Var[X]=\Var[Y]\leq \E[Y^2]=
\sum_{x,y,x',y'\in[u],x\neq y,x'\neq y'}\E[(s_x f_x[i_x=i_y]s_y f_y)
  (s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})].\] 
Consider any term in the
sum. Suppose some key, say $x$, is unique in the sense that
$x\not\in\{y,x',y'\}$. Then we can apply Lemma \ref{lem:remove-si}.
Given that $x\neq y$
and $x'\neq y'$, we have either $2$ or $4$ such unique keys. If all
4 keys are distinct, as in \req{eq:E}, we get
\begin{align*}
\E[(s_x f_x[i_x=i_y]s_y f_y)&
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]\\
&=\E[(s_x f_x[i_x=i_y]s_y f_y])]\E[(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]\\
&=(f_x f_y/p^2)(f_{x'} f_{y'}/p^2)=f_x f_yf_{x'} f_{y'}/p^4.
\end{align*}
The expected sum over all such terms is thus bounded
as 
\begin{align}
\sum_{{\rm distinct}\, x,y,x',y'\in[u]}&
\E[(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]\nonumber\\
&=\sum_{{\rm distinct}\,x,y,x',y'\in[u]}
f_xf_yf_{x'}f_{y'}/p^4<F_1^4/p^4\leq F_2^2 n^2/p^4.\label{eq:distinct}
\end{align}
The last inequalities 
used \req{eq:F1F2}. We also have to consider all the cases with two unique keys, e.g., $x$ 
and $x'$ unique while $y=y'$. Then using Lemma \ref{lem:remove-si} and \req{eq:coll-ell=r-1}, we get
\begin{align*}
\E[(s_x f_x[i_x=i_y]s_y f_y)&(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]\\
&=f_xf_{x'}f_y^2\E[s_xs_{x'} [i_x=i_{x'}=i_y]]\\
&=f_xf_{x'}f_y^2\E[s_{x'} [r-1=i_{x'}=i_y]]/p\\
&=f_xf_{x'}f_y^2\E[r-1=i_y]/p^2\\
    &<f_xf_{x'}f_y^2/(rp^2).
\end{align*}    
Summing over all terms with $x$ and $x'$ unique while $y=y'$, and
using \req{eq:F1F2} and $u\leq p$, we get 
\begin{align*}
  \sum_{{\rm distinct}\,x,x',y} f_xf_{x'}f_y^2 /(rp^2)&<
    F_1^2F_2/(rp^2)\leq F_2^2 n/(rp^2).
\end{align*}
There are four ways we can pick the two unique keys $a\in \{x,y\}$
and $b\in \{x',y'\}$, so we conclude that
\begin{equation}\label{eq:one-pair}
\sum_{\begin{array}{c}x,y,x',y'\in[u], x\neq y, x'\neq y',\\
{\rm two\ keys\ are\ unique}\end{array}}
\E[(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]\leq 4 F_2^2 n/(rp^2)
\end{equation}
Finally, we need to reconsider the terms with two pairs, that
is where $(x,y)=(x',y')$ or $(x,y)=(y',x')$. In
this case, $(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})=f_x^2f_y^2[i_x=i_y]$.
By \req{eq:coll-ell<r-1} and \req{eq:coll-ell=r-1},  we 
get 
\begin{align}
\sum_{\begin{array}{c}x,y,x',y'\in[u], x\neq y, x'\neq y',\\
(x,y)=(x',y')\,\vee\,(x,y)=(y',x')\end{array}}&
\E[(s_x f_x[i_x=i_y]s_y f_y)
(s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})]\nonumber\\
&=2\sum_{x,y\in[u],x\neq y}(f_x^2f_y^2)\Pr[i_x=i_y]\nonumber\\
&=2\sum_{x,y\in[u],x\neq y}(f_x^2f_y^2)(1+r/p^2)/r\nonumber\\
&=2(F_2^2-F_4)(1+r/p^2)/r\label{eq:two-pairs}
\end{align}
Adding up add \req{eq:distinct}, \req{eq:one-pair}, and
\req{eq:two-pairs}, we get 
\[\Var[Y]\leq 2(F_2^2-F_4)/r+F_2^2 (n^2/p^4+2/p^2+4 n/(rp^2)).\]
We have defined our parameters to satisfy $2\leq r\leq u/2\leq (p+1)/4$
where $r$, $u$, and $p+1$ are all powers of two. Also $n\leq u$, so it 
follows that
\begin{equation}
n/p\leq u/n\geq\geq 7/4\textnormal{ and }p/r\geq 7/2.\label{eq:ratios}
\end{equation}
In particular, $(n/p)^2\leq(4/7)^2<0.33$, so we conclude
that
\begin{equation}\label{eq:var-good1}
\Var[Y]<2(F_2^2-F_4)/r+F_2^2 (2.33+4 n/r)/p^2.
\end{equation}
Next we note that $F_4\geq F_2^2/n$ follows from \req{eq:F1F2} applied
to vector of squared values $(f_0^2,\ldots,f_{u-1}^2)$. Combined
with \req{eq:var-good1} and \req{eq:ratios}, we get
\begin{align*}
\Var[Y]&<2(F_2^2-F_4)/r+F_2^2 (2.33+4 n/r)/p^2\\
&\leq 2F_2^2/r+F_2^2(-2+2.33nr/p^2+4 n^2/p^2)\\
&<2F_2^2/r.
\end{align*}
This completes the proof of \req{eq:V-F2-p}  and hence
of Theorem \ref{thm:h-and-s-p}.
\section{Algorithms and analysis with arbitrary number of buckets}
We now consider the case where we want to hash into
a number of buckets. We will analyze the collision probability
with most uniform maps introduced in Section \ref{sec:most-uniform},
and later we will show how it can be used in connection with the
two-for-one hashing from Section \ref{sec:two-for-one}.

\subsection{Collision probability with most uniform distributions}
We have a hash function $h:U\fct Q$, but we want hash values in $R$, so
we need a map $\mu:Q\fct R$, and then use $\mu\circ h$ as
our hash function from $U$ to $R$. We normally assume that the hash values 
with $h$ are pairwise independent, that is, for any distinct $x$ and $y$,
the hash values $h(x)$ and $h(y)$ are independent, but then 
$\mu(h(x))$ and $\mu(h(y))$ are also independent. This means
that the collision probability can be calculated
as 
\[\Pr[\mu(h(x))=\mu(h(y))]=\sum_{i\in R}\Pr[\mu(h(x))=\mu(h(y))=i]=\sum_{i\in R}\Pr[\mu(h(x)=i)]^2.\]
This sum of squared probabilities attains is minimum value $1/|R|$
exactly when $\mu(h(x))$ is uniform in $R$. 

Let $q=|Q|$ and $r=|R|$. Suppose that $h$ is $2$-universal. Then
$h(x)$ is uniform in $Q$, and then we get the lowest collision
probability with $\mu\circ h$ if $\mu$ is most uniform as defined in
Section \ref{sec:most-uniform}, that is, the number of elements from
$Q$ mapping to any $i\in[r]$ is either $\floor{q/r}$ or
$\ceil{q/r}$. To calculate the collision probability,
Let $a\in[r]$ be such that $r$ divides $q+a$. Then the map $\mu$ maps
$\ceil{q/r}=(q+a)/r$ balls to $r-a$ buckets and
$\floor{q/r}=(q+a-r)/r$ balls to $a$ buckets. For a key $x\in [u]$, we
thus have $r-a$ buckets hit with probability $(1+a/q)/r$ and
$a$ buckets hit with probability $(1-(r-a)/q)/r$.
The collision probability is then
\begin{align}
\Pr[\mu(h(x))=&\mu(h(y))]= (r-a)((1+a/q)/r)^2+a((1-(r-a)r/q)/r)^2\nonumber\\
  &=\frac{(r-a)+(r-a)2a/q+(r-a)a^2/q^2+ a-a2(r-a)/p+a(r-a)^2/q^2}{r^2}\nonumber\\
  &=\frac{r +r a (r-a)/q^2}{r^2}=(1+a(r-a)/q^2)/r\leq \left(1+(r/(2q))^2\right)/r.\label{eq:coll-a}
  \end{align}
Note that the above calculation generalizes the one for \req{eq:coll} which
had $a=1$. We will think of $(r/(2q))^2$ as the general relative rounding
cost when we do not have any information about how $r$ divides $q$.

\subsection{Two-for-one hashing from uniform bits to arbitrary number of buckets}
We will now briefly discuss how would get the two-for-one hash
functions in count sketches with an arbitrary number $r$ of bins based
on a single $4$-universal hash function $h:[u]\fct [2^b]$.  We want to
construct the two hash functions $s:[u]\fct\{-1,+1\}$ and
$i:[u]\fct[r]$. As usual the results with uniform $b$-bit strings will
set the bar that we later compare with when from $h$ we get hash values that
are only uniform in $[2^b-1]$.

The construction of $s$ and $i$ is presented in 
Algorithm \ref{alg:b-bit-arb-r}.
\begin{algorithm}\label{alg:b-bit-arb-r}
  \caption{For key $x\in [u]$, compute $i(x)=i_x\in[r]$ and
    $s(x)=s_x\in\{-1,+1\}$.\rule{5ex}{0ex}
    Uses 4-universal $h:[u]\fct [2^b]$.}
  $h_x\gets h(x)$\hfill $\rhd\quad h_x$ has $b$ uniform bits\\
  $j_x\gets h_x\&(2^{b-1}-1)$\hfill $\rhd\quad j_x$ gets $b-1$ least 
   significant bits of $h_x$\\
  $i_x\gets (rj_x)\texttt{>>}(b-1)$\hfill $\rhd\quad i_x$ is most uniform
  in $[r]$\\
$a_x\gets h_x\texttt{>>}(b-1)$\hfill $\rhd\quad a_x$ gets the most significant bit of $h_x$\\
$s_x\gets 2b_x-1$\hfill $\rhd\quad s_x$ is uniform in $\{-1,+1\}$ and
independent of $i_x$.\\
\end{algorithm}  
The difference relative to Algorithm \ref{alg:h-and-s} is the computation
of $i_x$ where we now first pick out the $(b-1)$-bit string $j_x$ from
$h_x$, and then apply the most uniform map $(rj_x)\texttt{>>}(b-1)$
to get $i_x$. This does not affect $s_x$ which remains independent
of $i_x$. 

We now have to study the effect on our estimate error
\[Y=X-F_2\sum_{x,y\in[u],x\neq y} s_x f_x[i_x=i_y]s_y f_y.\]
The expectation is exactly as before. The only
change is in the analysis of the variance where
we before had each $i_x$ uniform in $[r]=[2^\ell]$, hence
$\Pr[i_x=i_y]=1/r$. Now have values uniform in $[2^{b-1}]$ mapped
most uniformly to $[r]$ for an arbitrary $r$. Then by \req{eq:coll-a},
we get $\Pr[i_x=i_y]=\left(1+(r/(2q))^2\right)/r$ where $q=2^{b-1}$. 
This increases
the variance bound accordingly from $2F_2^2/r$ to
\begin{equation}\label{eq:Var-b-bit-arb-r}
\Var[X]=\Var[Y]\leq 2F_2^2\left(1+(r/2^b)^2\right)/r.
\end{equation}

\subsection{Two-for-one hashing from Mersenne primes to arbitrary number of buckets}
We will now show how wan get the two-for-one hash functions in count
sketches with an arbitrary number $r$ of bins based on a single
$4$-universal hash function $h:[u]\fct [2^b-1]$.  Again we want to
construct the two hash functions $s:[u]\fct\{-1,+1\}$ and
$i:[u]\fct[r]$.  The construction will be the same as we had in
Algorithm \ref{alg:b-bit-arb-r} when $h$ returned uniform values in
$[2^b]$ with the change that we set $h_x\gets h(x)+1$, so that it
becomes uniform in $[2^b]\setminus\{0\}$. It is also convinient to
swap the sign of the signbit $s_x$ setting $s_x\gets 2a_x+1$ instead
of $s_x\gets 1-2a_x$. The basic reason is that we have swapped the
role of \texttt{0} and \texttt{1} in $a_x$.  The resulting algorithm
is presented as Algorithm \ref{alg:Mersenne-arb-r}.
\begin{algorithm}\label{alg:Mersenne-arb-r}
  \caption{For key $x\in [u]$, compute $i(x)=i_x\in[r]$ and
    $s(x)=s_x\in\{-1,+1\}$.\rule{5ex}{0ex}
    Uses 4-universal $h:[u]\fct [p]$ for Mersenne prime $p=2^b-1\geq u$.}
  $h_x\gets h(x)+1$\hfill $\rhd\quad h_x$ uses $b$ bits uniformly except $h_x\neq 0$\\
  $j_x\gets h_x\&(2^{b-1}-1)$\hfill $\rhd\quad j_x$ gets $b-1$ least 
   significant bits of $h_x$\\
  $i_x\gets (rj_x)\texttt{>>}b-1$\hfill $\rhd\quad i_x$ is quite uniform
  in $[r]$\\
$a_x\gets h_x\texttt{>>}(b-1)$\hfill $\rhd\quad a_x$ gets the most significant bit of $h_x$\\
$s_x\gets 1-2b_x$\hfill $\rhd\quad s_x$ is quite uniform in $\{-1,+1\}$ and
quite independent of $i_x$.\\
\end{algorithm}  
The rest of Algorithm \ref{alg:Mersenne-arb-r} is exactly like 
Algorithm \ref{alg:b-bit-arb-r}, and we will now discuss the new
distributions of the resulting variables. We had
$h_x$ uniform in $[2^b]\setminus\{0\}$, and then we set
$j_x \gets h_x\&(2^{b-1}-1)$. Then $j_x\in[2^{b-1}]$ with 
$\Pr[j_x=0]=1/(2^{b}-1)$ while  $\Pr[j_x=j]=2/(2^{b}-1)$ for all $j>0$.

Next we set $i_x\gets (rj_x)\texttt{>>}b-1$. We know from Lemma
\ref{lem:most-uniform} (i) that this is a most uniform map from
$[2^{b-1}]$ to $[r]$.  It maps a maximal number of elements from
$[2^{b-1}]$ to $0$, including $0$ which had half probability for
$j_x$. We conclude all $i\in[r]\setminus\{0\}$ have probability
$\floor{2^{b-1}/r}2/(2^{b}-1)$ or $\ceil{2^{b-1}/r}2/(2^{b}-1)$ while
$0$ has probability $(\ceil{2^{b-1}/r}2-1)/(2^{b}-1)$. 
We note
that the probability for $0$ is in the middle of the two other
bounds and often this yields a more uniform distribution on $[r]$ than
the most uniform distribution we could get from the
uniform distribuion on $[2^{b-1}]$.

With
more careful calculations, we can get some nicer bounds
that we shall later.
\begin{lemma}\label{lem:ix-r-dist} For any distinct $x,y\in [u]$, 
\begin{align}
\Pr[i_x=0]&=(1+r/2^b)/r\label{eq:ix=0}\\
\Pr[i_x=i_y]&\leq \left(1+(r/2^b)^2\right)/r.\label{eq:ix=iy}
\end{align}
\end{lemma}
\begin{proof}
The proof of \req{eq:ix=0} is a simple calculation. We
already argued that $\Pr[i_x=0]=(\ceil{2^{b-1}/r}2-1)/(2^{b}-1)$, so
\begin{align*}
\Pr[i_x=0]&\leq (2(2^{b-1}+r-1)/r)-1)/(2^{b}-1)\\
&=((2^b+r-2)/r)/(2^b-1)\\
&=\left(1+(r-1)/(2^b-1)\right)/r\\
&=\left(1+r/2^b\right)/r.\\
\end{align*}
Above the last inequality follows because $r<u<2^b$.

The proof of \ref{eq:ix=iy} follows from {\color{red}Thomas}.
\end{proof}
Lemma \ref{lem:ix-r-dist} above is all we need to know about the
marginal distribution of $i_x$. However, we also need a replacement
for Lemma \ref{lem:remove-si} for handling the signbit $s_x$.
\begin{lemma}\label{lem:remove-si-r-dist} Consider distinct keys
$x_1,\ldots,x_j$, $j\leq k$ and an expression $B=s_{x_1}A$ where $A$
depends on $i_{x_1},\ldots,i_{x_j}$ and $s_{x_2},\ldots,s_{x_j}$ but not
$s_{x_1}$. 
Then
\begin{equation}\label{eq:remove-si-r-dist}
\E[s_xA]=\E[A\suchthat i_x=0]/p.
\end{equation}
\end{lemma}
\begin{proof}
The proof follows the same idea as that for Lemma \ref{lem:remove-si}.
First we have
\[\E[B]=\E_{h_{x_1}\gets U([2^b]\setminus\{0\})}[B]=\E_{h_{x_1}\gets U[2^b]}[B]2^b/p-\E_{h_{x_1}=0}[B]/p.\]
With $h_{x_1}\gets U[2^b]$, the bit $a_{x_1}$ is uniform and 
independent of $j_{x_1}$, so $s_{x_1}\in\{-1,+1\}$ is uniform and 
independent of $i_{x_1}$, and therefore 
\[\E_{h_{x_1}\gets U[2^b]}[s_{x_1}A]=0.\]
Moreover, $h_{x_1}=0$ implies $j_x={x_1}$, $i_{x_1}=0$, $a_{x_1}=0$,
and $s_{x_1}=-1$,
so 
\[\E[s_{x_1}A]=-\E_{h_{x_1}=0}[s_{x_1}A]/p=\E_{i_{x_1}=0}[A].\]
\end{proof}
We now consider our $F_2$ extimator
\[X=\sum_{i\in[r]}\left( \sum_{x\in[u]}s_x f_x[i_x=i]\right)^2\!
=F_2+Y\mbox{ where }Y=\sum_{x,y\in[u],x\neq y}
s_x f_x[i_x=i_y]s_y f_y.\]
Using Lemma \ref{lem:remove-si-r-dist} instead of Lemma
\ref{lem:remove-si-r-dist} we get an almost identical calculation 
of the expection $\E[Y]$ as
in Section \ref{sec:two-for-one} and with the same
conclusion that 
\begin{equation}\label{eq:E-Mersenne-arb-r}
\E[Y]=(F_1^2-F_2)/p^2\leq (n-1)F_2/p^2.
\end{equation}
Therfore we still have the same bound \req{eq:E-F2-p} as in Theorem \ref{thm:h-and-s-p}.

Concerning the variance, we have some more changes to the 
calculations in Section \ref{sec:two-for-one}.
We still start with
\[\Var[X]=\Var[Y]\leq \E[Y^2]=
\sum_{x,y,x',y'\in[u],x\neq y,x'\neq y'}\E[(s_x f_x[i_x=i_y]s_y f_y)
  (s_{x'}f_{x'}[i_{x'}=i_{y'}]s_{y'} f_{y'})].\] 
For terms with 4 distinct keys, we still get the same expectation
as in Section \ref{sec:two-for-one}, so their
total contribution to the variance is still the $F_2^2 n^2/p^4$ from
\req{eq:distinct}

However, for the terms with two unique keys and one pair of identical
keys, the factor $\E[i_x=r-1]<1/r$ gets replaced with $\E[i_x=0]$
which by \req{eq:ix=0} in Lemma \ref{lem:ix-r-dist} is bound by
$(1+r/2^b)/r$. As a result, for the total contribution of these terms,
we have to multiply the $4 F_2^2 n/(rp^2)$ from \req{eq:one-pair} by
$(1+r/2^b)$, so they now sum up to
\begin{equation}\label{eq:one-pair'}
4 (1+r/2^b) F_2^2 n/(rp^2)
\end{equation}
Finally, for the terms with two pairs of identical keys, $\Pr[i_x=i_y]$ was bounded
by $(1+1/p)/r$, which is replaced by the bound $\left(1+(r/2^b)^2\right)/r$, so
so 
\begin{align}
2\sum_{x,y\in[u],x\neq y}&(f_x^2f_y^2)\Pr[i_x=i_y]\nonumber\\
&=2\sum_{x,y\in[u],x\neq y}(f_x^2f_y^2)(1+(r/2^b)^2)/k\nonumber\\
&=2(F_2^2-F_4)(1+(r/2^b)^2)/k\nonumber\\
&\leq 2(1-1/n)F_2^2(1+(r/2^b)^2)/k\label{eq:two-pairs'}
\end{align}
Adding it all up, we have proved that 
\[\Var[Y]\leq F_2^2 n^2/p^4+2(1-1/n)F_2^2(1+(r/2^b)^2)/r+4(1+r/2^b)F_2^2 n/(rp^2).\]
We want the argue that we get the same variance bound as we had in 
\req{eq:Var-b-bit-arb-r} with uniform $b$-bit hash values; namely that
\begin{equation}\label{eq:Var-Mersenne-arb-r}
\Var[Y]\leq 2\left(1+(r/2^b)^2\right)F_2^2/r.
\end{equation}
which follows if 
\[ru^3/p^4+4(1+r/2^b)n^2/p^2\leq 2.\]
Recall that $2\leq k<u\leq (p+1)/2$. Since $u$ is a power of two,
$u\geq 4$. We conclude $n/p\leq u/p\leq 4/7$, $r/p\leq 3/7$, and
$p+1\geq 8$. Therefore the left-hand side is bounded by
$(3/7)(4/7)^3+4(1+2/8)(4/7)^2<1.8<2$.  This completes the proof of
\req{eq:Var-Mersenne-arb-r}. The basic conclusion is that both with
$r$ is a power of two and when $r$ is arbitrary, we get the same
variance bounds with Mersenne primes as we did with uniform $b$-bit
strings. The only difference is the small error in expectation from
\req{eq:E-Mersenne-arb-r}. Relative to the correct value $F_2$, the
relative error in the expectation is by a factor of at most
$n/p^2$ which is insignificant when $p$ is large.



\bibliographystyle{alpha}
\bibliography{general}

\end{document}






We are now going to show we can implement second moment estimation when
we want an arbitrary number $r$ of counters. The starting point
is a 4-universal hash function $h:[u]\fct[2^b-1]$, and we want to
use it to construct both a random sign function $s:[u]\fct\{-1,1\}$ and 
a random bucketing function $i:[u]\fct[r]$. When $r$ was a power-of-two, 
we such a construction in Algorithm \ref{alg:h-and-s}, but now we
assume that $r$ is not a power of two. We want to employ what we
have learned about most uniform maps, but it has to be done a bit
carefully. The result is presented in Algorithm \ref{alg:h-and-s-k}.
\begin{algorithm}\label{alg:h-and-s-k}
  \caption{For key $x\in [u]$, compute $i(x)=i_x\in[r]$ and
    $s(x)=s_x\in\{-1,+1\}$.\rule{5ex}{0ex}
    Uses 4-universal $h:[u]\fct [p]$ for Mersenne prime $p=2^b-1\geq u$.}
  $h_x\gets h(x)+1$\hfill $\rhd\quad h_x$ uses $b$ bits uniformly except $h_x\neq 0$\\
  $j_x\gets h_x\&(2^{b-1}-1)$\hfill $\rhd\quad j_x$ gets $b-1$ least 
   significant bits of $h_x$\\
  $i_x\gets (rj_x)\texttt{>>}b-1$\hfill $\rhd\quad i_x$ is quite uniform
  in $[r]$\\
$a_x\gets h_x\texttt{>>}(b-1)$\hfill $\rhd\quad a_x$ gets the most significant bit of $h_x$\\
$s_x\gets 2b_x-1$\hfill $\rhd\quad s_x$ is quite uniform in $\{-1,+1\}$ and
quite independent of $i_x$.\\
\end{algorithm}  

We now explain and analyze Algorithm \ref{alg:h-and-s-k}. First we set
$h_x=h(x)+1$.  The result is $b$-bit string that is uniform except
that we are missing 0, the all \texttt0s. For contract, in Algorithm
\ref{alg:h-and-s}, $h_x$ was uniform except for the all
\texttt1s. 

Now $j_x$ gets the $b-1$ least significant bits of
$h_x$. We have two elements from $[2^b-1]$ mapping to each element in
$[2^{b-1}]$, except that only one element maps to $0$, so $0$ has only
half the probability. We note that this distribution is a weighted
average between the distribution that is uniform on $[2^{b-1}]$ and
the distribution that is uniform on $[2^{b-1}]\setminus\{0\}$.

Next we set $i_x\gets (rj_x)\texttt{>>}b-1$. This is the map that
to $[r]$ that we know from Lemma \ref{lem:most-uniform} (i)-(ii) 
is most uniform on both $[2^{b-1}]$ and $[2^{b-1}]\setminus\{0\}$.
The resulting distribution is therefore again a weighted average
between if $i_x$ was uniform on $[2^{b-1}]$ and
if $i_x$ was uniform on $[2^{b-1}]\setminus\{0\}$. By Jensen's inequality





We now consider the situation where balls in $[q]$ have unit weight while ball $q$
has weight $\eps\in[0,1]$. This corresponds to the case where we have a uniform
distribution in $[p]$ where $p=2^c-1$. This is a uniform $c$-bit string, except
that the all 1s $p$ is excluded. Now set $r=2^b-1$, and use $y=(x\texttt{\&} r)$.
This corresponds to ball $r$ getting weight $\eps=1-2^{b-c}$. We then do
our final mapping $z=(y+1)\texttt{>>} b$, which places ball $r$ in bin $0$.

Let $a\in\{1,\ldots,k\}$ be such that $k$ divides $q+a$. Then $a$ ``light'' buckets
get $\ell=(q+a-k)/k$ unit balls each while $k-a$ ``heavy'' buckets 
get $\ell+1=(q+a)/k$ unit balls each. We have here allowed $k$ to divide $r$.
Now, the special ball $r$ lands in bucket 0, which is was one of
the light buckets which gets total weight $\ell+\eps$. 

If the weight the balls landing in a bin $i$ is $w_i$, then the probability
of landing in it is $w_i/(r+\eps)$. We now want to
analyse the collision probability, which is $\sum_{i\in k} w_i^2/(r+\eps)$.
We already know the answer if $\eps$ is $0$ or $1$. The former corresponds
to case of a uniform distribution in $r$, and the latter to a uniform
distribution in $r+1$. For any $\eps$, we can get our distribution as
a weighted average of these two. We want ball $r$ to get probability $\eps/(r+\eps)$.
Ball $r$ gets zero probability from the uniform distribution on $[r]$, and it gets
probability $1/(r+1)$ from the uniform distribution of on $[r+1]$, so we
must use the later with weight $\alpha$ such that 
\[\alpha/(r+1)=\eps/(r+\eps)\iff \alpha=\eps\frac{r+1}{r+\eps}.\]
This weighted average gives us the right distribution on balls, hence the
right distribution on bins. We want the sum of squares for this average distribution,
and by Jensen's inequality, this is less than or equal to the corresponding
weigthed average of the sum of squares. Thus we
get that the collision probability with the $\eps$-weighted ball $r$ is
bounded by 
\begin{align*}
(1-\alpha)(1+&a(k-a)/r^2)/k+\alpha (1+(a-1)(k-(a-1))/(r+1)^2)/k\\
&=(1+(1-\alpha)(a(k-a)/r^2)+\alpha (1+(a-1)(k-(a-1))/(r+1)^2))/k.
\end{align*}
However, all we care about here is that it is bounded by the maximum 
of the two extremes, which are both most uniform distributions
and both are bounded by $(1+(k/(2r))^2)/k$, that is,
\begin{equation}\label{eq:collision-r}
\Pr[i_x=i_y]=(1+(k/(2r))^2)/k.
\end{equation}
Best use of $r$ is to use $c=b-1$ so that all bits except the sign bit is used.
Then $r=(p+1)/2-1=(p-1)/2$, so we get
\begin{equation}\label{eq:collision-p}
\Pr[i_x=i_y]=(1+(k/((p-1))^2)/k.
\end{equation}






The construction of $s$ abd $i$ is presented in Algorithm \ref{alg:i-and-s-from-b-bit}.




\subsection{Using fewer hash bits to spread the keys}
Recall from Section \ref{sec:two-for-one} that we sometimes want to
get several hash values out of the same $k$-universal hash function
$h:[u]\fct[2^b-1]$. Our goal is to get a hash value $i(x)\in [r]$ for
some arbitrary $r$, but we only want to use $c<b$ bits from the
$b$-bit hash value. An example of such a code using the $c$ least
significant bits is presented in Algorithm \ref{alg:i-use-c-from h}.
\begin{algorithm}\label{alg:i-use-c-from h}
  \caption{For key $x\in [u]$, compute $i(x)=i_x\in[r]$.
    Uses $c$
    least significant bits of hash values from 4-universal $h:[u]\fct
    [2^b-1]$.}  
   $h_x\gets h(x)+1$\hfill $\rhd\quad h_x$ uses $b$ bits
  uniformly except $h_x\neq 0$\\ 
  $j_x\gets h_x\&(2^c-1)$\hfill
  $\rhd\quad j_x$ gets $c$ least significant bits of
  $h_x$\\ $i_x\gets (rj_x)\texttt{>>}\,c$\hfill $\rhd\quad i_x$ is
  quite uniform in $[r]$.
\end{algorithm}  
To compute $i(x)$ from $h(x)$, we first set 
$h_x\gets h(x)+1$. The result is $b$-bit string that is uniform except
that we are missing 0. Next we can select any $c$ bits from $h_x$
for $j_x$, e.g., we could set $j_x\gets h_x\texttt\&(2^c-1)$ for
the $c$ least significant bits, but we could also have chosen
$j_x\gets h_x\texttt{>>}(b-c)$ for the $c$ most signifant bits.
Regardless of which bits we chose $j_x$ is uniform in $[2^c]$
except that $0$ has smaller probability. More precisely, 
$0$ has probability $(2^{b-c}-1)/(2^c-1)$ while the all
other values in $[2^c]$ have probability $2^{b-c}/(2^c-1)$.
It will be more convinient to think of it relatively; that
the probability of $0$ is $(1-\eps)$ times smaller than that of 
the rest. Concretely $\eps=2^{c-b}$, but we will think of $\eps$ as
any value in $[0,1]$. If $\eps=0$, then $j_x$ is
uniformly distributed in $[2^c]$ and if
$\eps=1$, then $j_x$ is uniformly distributed in $2^c\setminus\{0\}$.
All other values of $\eps$ give us some weighted average between
these two extremes.

Finally, we set $i_x\gets (rj_x)\texttt{>>}c$ defining $i(x)=i_x$.
\begin{lemma} For any $x\in[u]$ and $i\in[r]$, 
$\Pr[h(x)=i] \in[\floor{(2^c-\eps)/r}/(2^c-\eps), \ceil{(2^c-\eps)/r}/(2^c-\eps)]$. Moreover, 
$\Pr[i(x)=0]=(\ceil{2^c/r}-\eps)/(2^c-\eps)\leq \ceil{2^c/r}/2^c$. Moreover, for 
Moreover, for any distinct $x,y\in [u]$, $\Pr[i(x)=i(y)]\leq \left(1+(r/(2(2^c-1)))^2\right)/r$.
\end{lemma}
Above $\ceil{2^c/r}/2^c\leq\left(1+(r-1)/2^c\right)/r$


This is the map 
to $[r]$ that we know from Lemma \ref{lem:most-uniform} (i)-(ii) 
is most uniform on both $[2^c]$ and $[2^c]\setminus\{0\}$. We
can therefore apply \req{eq:coll-a} in these two 
extremes. It follows that if $j_x$ was uniform in 
$[2^c]$, then the sum of squared probabilities on $i_x$ is
at most $(1+(r/(2\cdot 2^c))^2)/r$, and if $j_x$ was uniform in 
$[2^c]\setminus\{0\}$, then the sum of squared probabilities on $i_x$ is
at most $(1+(r/(2\cdot (2^c-1)))^2)/r$.

Therefore, if $j_x$ was uniform in $[2^c]$ then by  \ref{lem:most-uniform} (i),



The resulting distribution is therefore again a weighted average
between if $i_x$ was uniform on $[2^c]$ and
if $i_x$ was uniform on $[2^c]\setminus\{0\}$. By Jensen's inequality




\subsection{Two-for-one hash functions with an arbitrary number of balls}
We are now going to show we can implement second moment estimation when
we want an arbitrary number $r$ of counters. The starting point
is a 4-universal hash function $h:[u]\fct[2^b-1]$, and we want to
use it to construct both a random sign function $s:[u]\fct\{-1,1\}$ and 
a random bucketing function $i:[u]\fct[r]$. When $r$ was a power-of-two, 
we such a construction in Algorithm \ref{alg:h-and-s}, but now we
assume that $r$ is not a power of two. We want to employ what we
have learned about most uniform maps, but it has to be done a bit
carefully. The result is presented in Algorithm \ref{alg:h-and-s-k}.
\begin{algorithm}\label{alg:h-and-s-k}
  \caption{For key $x\in [u]$, compute $i(x)=i_x\in[r]$ and
    $s(x)=s_x\in\{-1,+1\}$.\rule{5ex}{0ex}
    Uses 4-universal $h:[u]\fct [p]$ for Mersenne prime $p=2^b-1\geq u$.}
  $h_x\gets h(x)+1$\hfill $\rhd\quad h_x$ uses $b$ bits uniformly except $h_x\neq 0$\\
  $j_x\gets h_x\&(2^{b-1}-1)$\hfill $\rhd\quad j_x$ gets $b-1$ least 
   significant bits of $h_x$\\
  $i_x\gets (rj_x)\texttt{>>}b-1$\hfill $\rhd\quad i_x$ is quite uniform
  in $[r]$\\
$a_x\gets h_x\texttt{>>}(b-1)$\hfill $\rhd\quad a_x$ gets the most significant bit of $h_x$\\
$s_x\gets 2b_x-1$\hfill $\rhd\quad s_x$ is quite uniform in $\{-1,+1\}$ and
quite independent of $i_x$.\\
\end{algorithm}  

We now explain and analyze Algorithm \ref{alg:h-and-s-k}. First we set
$h_x=h(x)+1$.  The result is $b$-bit string that is uniform except
that we are missing 0, the all \texttt0s. For contract, in Algorithm
\ref{alg:h-and-s}, $h_x$ was uniform except for the all
\texttt1s. 

Now $j_x$ gets the $b-1$ least significant bits of
$h_x$. We have two elements from $[2^b-1]$ mapping to each element in
$[2^{b-1}]$, except that only one element maps to $0$, so $0$ has only
half the probability. We note that this distribution is a weighted
average between the distribution that is uniform on $[2^{b-1}]$ and
the distribution that is uniform on $[2^{b-1}]\setminus\{0\}$.

Next we set $i_x\gets (rj_x)\texttt{>>}b-1$. This is the map that
to $[r]$ that we know from Lemma \ref{lem:most-uniform} (i)-(ii) 
is most uniform on both $[2^{b-1}]$ and $[2^{b-1}]\setminus\{0\}$.
The resulting distribution is therefore again a weighted average
between if $i_x$ was uniform on $[2^{b-1}]$ and
if $i_x$ was uniform on $[2^{b-1}]\setminus\{0\}$. By Jensen's inequality





We now consider the situation where balls in $[q]$ have unit weight while ball $q$
has weight $\eps\in[0,1]$. This corresponds to the case where we have a uniform
distribution in $[p]$ where $p=2^c-1$. This is a uniform $c$-bit string, except
that the all 1s $p$ is excluded. Now set $r=2^b-1$, and use $y=(x\texttt{\&} r)$.
This corresponds to ball $r$ getting weight $\eps=1-2^{b-c}$. We then do
our final mapping $z=(y+1)\texttt{>>} b$, which places ball $r$ in bin $0$.

Let $a\in\{1,\ldots,k\}$ be such that $k$ divides $q+a$. Then $a$ ``light'' buckets
get $\ell=(q+a-k)/k$ unit balls each while $k-a$ ``heavy'' buckets 
get $\ell+1=(q+a)/k$ unit balls each. We have here allowed $k$ to divide $r$.
Now, the special ball $r$ lands in bucket 0, which is was one of
the light buckets which gets total weight $\ell+\eps$. 

If the weight the balls landing in a bin $i$ is $w_i$, then the probability
of landing in it is $w_i/(r+\eps)$. We now want to
analyse the collision probability, which is $\sum_{i\in k} w_i^2/(r+\eps)$.
We already know the answer if $\eps$ is $0$ or $1$. The former corresponds
to case of a uniform distribution in $r$, and the latter to a uniform
distribution in $r+1$. For any $\eps$, we can get our distribution as
a weighted average of these two. We want ball $r$ to get probability $\eps/(r+\eps)$.
Ball $r$ gets zero probability from the uniform distribution on $[r]$, and it gets
probability $1/(r+1)$ from the uniform distribution of on $[r+1]$, so we
must use the later with weight $\alpha$ such that 
\[\alpha/(r+1)=\eps/(r+\eps)\iff \alpha=\eps\frac{r+1}{r+\eps}.\]
This weighted average gives us the right distribution on balls, hence the
right distribution on bins. We want the sum of squares for this average distribution,
and by Jensen's inequality, this is less than or equal to the corresponding
weigthed average of the sum of squares. Thus we
get that the collision probability with the $\eps$-weighted ball $r$ is
bounded by 
\begin{align*}
(1-\alpha)(1+&a(k-a)/r^2)/k+\alpha (1+(a-1)(k-(a-1))/(r+1)^2)/k\\
&=(1+(1-\alpha)(a(k-a)/r^2)+\alpha (1+(a-1)(k-(a-1))/(r+1)^2))/k.
\end{align*}
However, all we care about here is that it is bounded by the maximum 
of the two extremes, which are both most uniform distributions
and both are bounded by $(1+(k/(2r))^2)/k$, that is,
\begin{equation}\label{eq:collision-r}
\Pr[i_x=i_y]=(1+(k/(2r))^2)/k.
\end{equation}
Best use of $r$ is to use $c=b-1$ so that all bits except the sign bit is used.
Then $r=(p+1)/2-1=(p-1)/2$, so we get
\begin{equation}\label{eq:collision-p}
\Pr[i_x=i_y]=(1+(k/((p-1))^2)/k.
\end{equation}


\subsection{Other stuff}
We now have $k< u$ be arbitrary and pick Mersenne prime $p=2^b-1\geq 10 u$.

\begin{algorithm}\label{alg:h-and-s-k}
  \caption{For key $x\in [u]$, compute $h(x)=i_x\in[k]$ and
    $s(x)=s_x\in\{-1,+1\}$.\rule{5ex}{0ex}
    Uses 4-universal $g:[u]\fct [p]$ for Mersenne prime $p=2^b-1\geq u$..}
  $g_x\gets g(x)$\hfill $\rhd\quad g_x$ uses $b$ bits\\
  $f_x\gets g_x\&(2^{b-1}-1)$\hfill $\rhd\quad f_x$ gets $b-1$ least significant
    bits of $g_x$\\
  $i_x\gets k(f_x+1)\texttt{>>}b-1$\hfill $\rhd\quad i_x$ is most uniform
  in $[k]$\\
$b_x\gets g_x\texttt{>>}(b-1)$\hfill $\rhd\quad b_x$ gets the most significant bit of $g_x$\\
$s_x\gets 1-2b_x$
\end{algorithm}  
From \req{eq:remove-si}, we have
\begin{equation*}
\E[s_xA]=\E_{g_x=p}[A]/p.
\end{equation*}
With our old calculation of $g_x=p$ implied $i_x=r-1$. However, now 
$g_x=p$ implies $i_x=0$, so Lemma \ref{lem:remove-si} is replaced
by 
\begin{lemma}\label{lem:remove-si-k}
$\E[s_xA]=\E[A\suchthat i_x=0]/p$.
\end{lemma}
The calculation of the expection is unchanged, and the same with the
calculation of variance for terms with 4 distinct keys, so their
total contribution to the variance is still the $F_2^2 n^2/p^4$ from
\req{eq:distinct}

However, for the terms with two unique keys and one pair of identical keys, the
factor $\E[i_x=r-1]<1/k$ gets replaced with $\E[i_x=0]$ with our
new probability distribution where
\begin{align*}
\Pr[i_x=0]&=(\floor{r/k}+\eps)/(r+\eps)\leq \floor{r/k}+1)/(r+1)\\
&\leq (r/k+1)/(r+1)<(1+k/(r+1))/k=(1+2k/(p+1))/k.
\end{align*}
As a result, for the total contribution of these terms, we have to multiply the
$4 F_2^2 n/(rp^2)$ from \req{eq:one-pair} by $(1+2k/(p+1))$, so they
sum up to
\begin{equation}\label{eq:one-pair'}
4 (1+2k/(p+1)) F_2^2 n/(rp^2)
\end{equation}
Finally, for the terms with two pairs of identical keys, $\Pr[i_x=i_y]$ was bounded
by $(1+1/p)/k$, which is replaced by the bound $(1+(k/(p-1))^2)/k$, so
so 
\begin{align}
2\sum_{x,y\in[u],x\neq y}&(f_x^2f_y^2)\Pr[i_x=i_y]\nonumber\\
&=2\sum_{x,y\in[u],x\neq y}(f_x^2f_y^2)(1+(2k/(p-1))^2)/k\nonumber\\
&=2(F_2^2-F_4)(1+(k/(p-1))^2)/k\nonumber\\
&\leq 2(1-1/n)F_2^2(1+(k/(p-1))^2)/k\label{eq:two-pairs'}\\
\end{align}
Adding it all up, we have proved that 
\[\Var[Y]=F_2^2 n^2/p^4+(1-1/n)F_2^2(1+(k/(p-1))^2)/k+4(1+2/(p+1))F_2^2 n/(rp^2).\]
We want the argue
\begin{equation}\label{eq:Var-bound}
\Var[Y]\leq 2(1+(k/(p-1))^2)))F_2^2/k.
\end{equation}
which follows if 
\[ku^3/p^4+4(1+2/(p+1))n^2/p^2\leq 2(1+(k/(p-1))^2)).\]
In fact, we will show 
\[ku^3/p^4+4(1+2/(p+1))n^2/p^2\leq 2.\]
Recall that $2\leq k<u\leq (p+1)/2$. Since $u$ is a power of two,
$u\geq 4$. We conclude $n/p\leq u/p\leq 4/7$, $r/p\leq 3/7$, and
$p+1\geq 8$. Therefore the left-hand side is bounded by
$(3/7)(4/7)^3+4(1+2/8)(4/7)^2<1.8<2$. This completes the proof of
\req{eq:Var-bound}.  \bibliographystyle{alpha} \bibliography{general}

\end{document}
\section{Abitrary number of bins}
We say a map $R$ to $[k]$ is \emph{most uniform} if for any
$i\in[k]$ the number of elements from $R$ mapping to $i$ is either
$\floor {|R|/k}$ or $\ceil{|R|/k}$. 

For example, with $R=[r]$, $x\mapsto x\bmod k$ is a most uniform map
from $[r]$ to $[k]$. However, the modulo operation is quite slow on
many computers, unless say, $k$ is a power of two, in which case it
can be implemented as $x\texttt\& (r-1)$ where $\texttt\&$ denotes
bit-wise {\sc and}.

Another example of a most uniform map is $x\mapsto \floor{xk/r}$,
which is also quite slow in general, but if $r=2^b$ is a power of two,
it can be implemented as $x\mapsto x\texttt{>>} b$ where $\texttt{>>}$ denotes
right-shift.

Now, our interest is the case where $r=p=2^b-1$ is a Mersenne prime,
and here we claim  that
\[x\mapsto \floor{(x+1)k/2^b}=(x+1)\texttt{>>} b \]
is most uniform. The proof is simple. The statement is equivalent
to saying that $x\mapsto\texttt{>>} b$ is most uniform on 
$\{1,\ldots,r\}$, but we know it is most uniform on $[2^b]=\{0,\ldots,r\}$
in that every $i\in[k]$ gets hit by $\floor {2^b/k}$ or $\ceil{2^b/k}$
elements from $[2^b]$. It is also that $\ceil{2^b/k}$ elements, including
$0$, maps to $0$. Removing $0$ implies that only $\ceil{2^b/k}-1$ elements map
to $0$.

Now $k$ cannot divide $p=2^b-1$ since $p$ is prime, so 
$\ceil{p/k}=\ceil{p+1/k}=\ceil{2^b/k}$
and $\floor{p/k}=\ceil{p/k}-1$. Therefore we conclude $x\mapsto\texttt{>>} b$ 
maps $\ceil{p/k}$ or $\floor{p/k}$ elements from $\{1,\ldots,2^b-1\}$ 
to each $i\in[k]$, and the same holds for $x\mapsto (x+1)\texttt{>>} b$ applied
to $[p]$, so this efficient map is most uniform from $[p]$ to $[k]$.

We note that this trick does not work when $r=2^b-a$ for $a\geq 2$, that is,
using $x\mapsto (x+a)\texttt{>>} b$, for in the general case, the number of elements 
hashing to $0$ becomes $\floor {2^b/k}-a$ assuming $a\leq \floor {2^b/k}$; 0 otherwise.

\end{document}
(1-1/u)(1+((k/(p-1))^2)<?(1+((k/(p+1))^2)

It is less than 1+((k/(p-1))^2)-1/u, and we ask
if ((k/(p-1))^2)-((k/(p+1))^2)<=1/u=2/(p+1)

Putting it all on one, we want k^2((p+1)^2-(p-1)^2)<2(p+1)(p-1)^2.

Worst case k=1... not true, all wrong ??... so k^2((p+1)^2-(p-1)^2)<2(p+1)(p-1)^2.
This becomes 4p^2<2(p^3-p^2-p+1)... Very true since p>=7.

p




(k(p+1)/3/(p-1))^2)-1/u, and putting together,






MUCH EASIER if ((x+1)*k)>>b is a most uniform map from [2^b-1] to [k]

Cool trick... set $g_x\gets g_x+1$, so it is distributed in $\{1,\ldots,p\}$,
hence is it the all zeros that has smaller probability, which is useful.
Then if we do $(g_x\cdot k)\texttt b$, we get a good distribution in
the sence that for any $i \in [k]$, 
\[\floor{p/k}/p\leq \Pr[h(x)=i]\leq \ceil{p/k}/p.\]
The interesting thing is that $\Pr[h(x)=0]$ lies strictly between these values.

NOT QUITE TRUE, BUT SOMETHING LIKE IT...



\end{document}

